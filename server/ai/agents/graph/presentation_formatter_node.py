"""
Presentation Formatter Node
í”„ë¡ íŠ¸ì—”ë“œìš© ë°ì´í„° ì¬êµ¬ì„± + LLM ë°°ì¹˜ ê·¼ê±° ì¬ìƒì„±
"""

import json
from typing import Dict, List
from datetime import datetime
from openai import AsyncOpenAI
from pathlib import Path


class PresentationFormatter:
    """
    í”„ë¡ íŠ¸ì—”ë“œìš© ë°ì´í„° ë³€í™˜ê¸°
    
    í•µì‹¬ ê¸°ëŠ¥:
        1. LLM ë°°ì¹˜ í˜¸ì¶œë¡œ ëª¨ë“  ì—­ëŸ‰ ê·¼ê±° í•œ ë²ˆì— ì¬ìƒì„±
        2. Strengths/Weaknesses/Key_observations í‰ì„œí˜• ë³€í™˜
        3. Resume ê²€ì¦ ê²°ê³¼ í¬í•¨
        4. ì—­ëŸ‰ë³„ ê·¼ê±° ê·¸ë£¹í•‘ (segment ì—¬ëŸ¬ ê°œ)
        5. Transcript ë§¤í•‘ ì •ë³´ í¬í•¨
        6. ì§ë¬´/ê³µí†µ ì ìˆ˜ ê³„ì‚°
    """
    
    # ì—­ëŸ‰ëª… í•œê¸€ ë§¤í•‘
    COMPETENCY_DISPLAY_NAMES = {
        "achievement_motivation": "ì„±ì·¨/ë™ê¸° ì—­ëŸ‰",
        "growth_potential": "ì„±ì¥ ì ì¬ë ¥",
        "interpersonal_skill": "ëŒ€ì¸ê´€ê³„ ì—­ëŸ‰",
        "organizational_fit": "ì¡°ì§ ì í•©ì„±",
        "problem_solving": "ë¬¸ì œ í•´ê²°",
        "customer_journey_marketing": "ê³ ê° ì—¬ì • ë§ˆì¼€íŒ…",
        "md_data_analysis": "MD ë°ì´í„° ë¶„ì„",
        "seasonal_strategy_kpi": "ì‹œì¦Œ ì „ëµ KPI",
        "stakeholder_collaboration": "ì´í•´ê´€ê³„ì í˜‘ì—…",
        "value_chain_optimization": "ê°€ì¹˜ì‚¬ìŠ¬ ìµœì í™”",
    }
    
    # ì—­ëŸ‰ ê·¸ë£¹ ì •ì˜
    JOB_COMPETENCIES = [
        "customer_journey_marketing",
        "md_data_analysis", 
        "seasonal_strategy_kpi",
        "stakeholder_collaboration",
        "value_chain_optimization"
    ]
    
    COMMON_COMPETENCIES = [
        "achievement_motivation",
        "growth_potential",
        "interpersonal_skill",
        "organizational_fit",
        "problem_solving"
    ]
    
    
    def __init__(self, openai_client: AsyncOpenAI):
        self.client = openai_client
        self._transcript_data = None
    
    
    async def format(
        self,
        final_result: Dict,
        aggregated_competencies: Dict,
        competency_weights: Dict[str, float],
        transcript: Dict 
    ) -> Dict:
        """
        í”„ë¡ íŠ¸ì—”ë“œìš© ì‘ë‹µ ìƒì„±
        
        Returns:
            {
                "overall_summary": {...},
                "score_breakdown": {...},
                "competency_scores": [...],
                "competency_details": {...}  
            }
        """
        
        print("\n[Presentation Formatter] ê·¼ê±° ë°°ì¹˜ ì¬ìƒì„± ì‹œì‘...")
        
        # 1. ì „ì²´ ìš”ì•½
        overall_summary = self._extract_overall_summary(final_result)
        
        # 2. ì ìˆ˜ ë¶„í•´ (ì „ì²´/ì§ë¬´/ê³µí†µ)
        score_breakdown = self._calculate_score_breakdown(
            aggregated_competencies,
            competency_weights,
            final_result
        )
        
        # 3. ì—­ëŸ‰ë³„ ì ìˆ˜
        competency_scores = final_result.get("competency_scores", [])
        
        # 4. ë°°ì¹˜ë¡œ ëª¨ë“  ì—­ëŸ‰ ì¬ìƒì„± (LLM 1íšŒ í˜¸ì¶œ)
        #    - Evidences
        #    - Strengths (í‰ì„œí˜•)
        #    - Weaknesses (í‰ì„œí˜•)
        #    - Key_observations (í‰ì„œí˜•)
        print(f"  10ê°œ ì—­ëŸ‰ì˜ ê·¼ê±°/ê°•ì /ì•½ì /ê´€ì°°ì„ 1ë²ˆì˜ LLM í˜¸ì¶œë¡œ ë°°ì¹˜ ìƒì„± ì¤‘...")
        
        batch_result = await self._regenerate_all_batch(
            aggregated_competencies,
            transcript
        )
        
        # 5. ì—­ëŸ‰ë³„ ìƒì„¸ êµ¬ì„±
        competency_details = {}
        
        for comp_name, comp_data in aggregated_competencies.items():
            comp_batch = batch_result.get(comp_name, {})
            
            competency_details[comp_name] = {
                "competency_display_name": self.COMPETENCY_DISPLAY_NAMES.get(comp_name, comp_name),
                "overall_score": comp_data.get("overall_score"),
                "confidence_v2": comp_data.get("confidence_v2"),
                
                # í‰ì„œí˜•ìœ¼ë¡œ ì¬ìƒì„±
                "strengths": comp_batch.get("strengths", comp_data.get("strengths", [])),
                "weaknesses": comp_batch.get("weaknesses", comp_data.get("weaknesses", [])),
                "key_observations": comp_batch.get("key_observations", comp_data.get("key_observations", [])),
                
                # ì¬ìƒì„±ëœ ê·¼ê±°
                "evidences": comp_batch.get("evidences", []),
                
                # Resume ê²€ì¦ ê²°ê³¼
                "resume_verification_summary": comp_data.get("resume_verification_summary", {
                    "verified_count": 0,
                    "high_strength_count": 0,
                    "key_evidence": []
                })
            }
        
        total_evidences = sum(len(cd.get('evidences', [])) for cd in competency_details.values())
        print(f"  ë°°ì¹˜ ìƒì„± ì™„ë£Œ: ì´ {total_evidences}ê°œ ê·¼ê±°")
        
        return {
            "overall_summary": overall_summary,
            "score_breakdown": score_breakdown,
            "competency_scores": competency_scores,
            "competency_details": competency_details
        }
    
    
    def _extract_overall_summary(self, final_result: Dict) -> Dict:
        """ì „ì²´ ìš”ì•½ ì¶”ì¶œ"""
        return {
            "final_score": final_result.get("final_score"),
            "avg_confidence": final_result.get("avg_confidence"),
            "reliability": final_result.get("reliability"),
            "overall_evaluation_summary": final_result.get("overall_evaluation_summary")
        }
    
    
    def _calculate_score_breakdown(
        self,
        aggregated_competencies: Dict,
        competency_weights: Dict[str, float],
        final_result: Dict
    ) -> Dict:
        """
        ì ìˆ˜ ë¶„í•´ ê³„ì‚° (ì „ì²´/ì§ë¬´/ê³µí†µ)
        """
        
        # ì§ë¬´ ì—­ëŸ‰ ì ìˆ˜
        job_total = 0.0
        job_weight_sum = 0.0
        
        for comp_name in self.JOB_COMPETENCIES:
            if comp_name in aggregated_competencies:
                score = aggregated_competencies[comp_name].get("overall_score", 0)
                weight = competency_weights.get(comp_name, 0)
                job_total += score * weight
                job_weight_sum += weight
        
        job_score = round(job_total / job_weight_sum, 1) if job_weight_sum > 0 else 0.0
        
        # ê³µí†µ ì—­ëŸ‰ ì ìˆ˜
        common_total = 0.0
        common_weight_sum = 0.0
        
        for comp_name in self.COMMON_COMPETENCIES:
            if comp_name in aggregated_competencies:
                score = aggregated_competencies[comp_name].get("overall_score", 0)
                weight = competency_weights.get(comp_name, 0)
                common_total += score * weight
                common_weight_sum += weight
        
        common_score = round(common_total / common_weight_sum, 1) if common_weight_sum > 0 else 0.0
        
        return {
            "final_score": final_result.get("final_score"),
            "job_score": job_score,
            "common_score": common_score,
            "job_competencies": self.JOB_COMPETENCIES,
            "common_competencies": self.COMMON_COMPETENCIES
        }
    
    
    async def _regenerate_all_batch(
        self,
        aggregated_competencies: Dict,
        transcript: Dict
    ) -> Dict[str, Dict]:

        self.transcript = transcript
    
        """
        ë°°ì¹˜: ëª¨ë“  ì—­ëŸ‰ì˜ ê·¼ê±°/ê°•ì /ì•½ì /ê´€ì°°ì„ 1ë²ˆì˜ LLM í˜¸ì¶œë¡œ ì¬ìƒì„±
        
        Returns:
            {
                "achievement_motivation": {
                    "evidences": [...],
                    "strengths": ["í‰ì„œí˜• ê°•ì 1", "í‰ì„œí˜• ê°•ì 2", ...],
                    "weaknesses": ["í‰ì„œí˜• ì•½ì 1", ...],
                    "key_observations": ["í‰ì„œí˜• ê´€ì°°1", ...]
                },
                ...
            }
        """
        
        # 1. ëª¨ë“  ì—­ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
        all_competencies_data = []
        
        for comp_name, comp_data in aggregated_competencies.items():
            perspectives = comp_data.get("perspectives", {})
            evidence_details = perspectives.get("evidence_details", [])

            for ev in evidence_details:
                segment_id = ev.get("segment_id")
                char_index = ev.get("char_index")
                
                # char_length í™•ì¸ ë° ê³„ì‚°
                if "char_length" not in ev or ev["char_length"] is None:
                    ev["char_length"] = len(ev.get("text", ""))
                
                # Transcriptì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ê°€
                if segment_id and char_index is not None:
                    actual_text = self._get_transcript_text(segment_id, char_index)
                    ev["actual_transcript_text"] = actual_text

            all_competencies_data.append({
                "competency_name": comp_name,
                "competency_display_name": self.COMPETENCY_DISPLAY_NAMES.get(comp_name, comp_name),
                "overall_score": comp_data.get("overall_score", 0),
                
                # Evidence ë°ì´í„°
                "evidence_details": perspectives.get("evidence_details", []),
                "evidence_reasoning": perspectives.get("evidence_reasoning", ""),
                
                # Behavioral ë°ì´í„°
                "behavioral_pattern": perspectives.get("behavioral_pattern", {}),
                "behavioral_reasoning": perspectives.get("behavioral_reasoning", ""),
                
                # Critical ë°ì´í„°
                "red_flags": perspectives.get("red_flags", []),
                "critical_reasoning": perspectives.get("critical_reasoning", ""),
                
                # ì›ë³¸ Strengths/Weaknesses/Key_observations
                "original_strengths": comp_data.get("strengths", []),
                "original_weaknesses": comp_data.get("weaknesses", []),
                "original_key_observations": comp_data.get("key_observations", [])
            })
        
        # 1.5. transcriptì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ê°€
        for comp_data in all_competencies_data:
            evidence_details = comp_data["evidence_details"]
            
            for ev in evidence_details:
                segment_id = ev.get("segment_id")
                char_index = ev.get("char_index")
                
                # transcriptì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                if segment_id and char_index is not None:
                    actual_text = self._get_transcript_text(segment_id, char_index)
                    ev["actual_transcript_text"] = actual_text
        
        # 2. ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_comprehensive_batch_prompt(all_competencies_data)
        
        # 3. LLM 1íšŒ í˜¸ì¶œ
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert at synthesizing competency evaluation data into clear, professional summaries for HR reports."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=12000,  # ê·¼ê±°+ê°•ì +ì•½ì +ê´€ì°° ëª¨ë‘ í¬í•¨ì´ë¯€ë¡œ í† í° ë” ë§ì´ í•„ìš”
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content.strip()
            result = json.loads(result_text)
            
            # 4. ì—­ëŸ‰ë³„ë¡œ íŒŒì‹±
            batch_by_competency = {}
            
            for comp_result in result.get("competencies", []):
                comp_name = comp_result.get("competency_name")
                batch_by_competency[comp_name] = {
                    "evidences": comp_result.get("evidences", []),
                    "strengths": comp_result.get("strengths", []),
                    "weaknesses": comp_result.get("weaknesses", []),
                    "key_observations": comp_result.get("key_observations", [])
                }
            
            return batch_by_competency
        
        except Exception as e:
            print(f"        ë°°ì¹˜ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # Fallback: ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return self._fallback_all_batch(aggregated_competencies)
    
    
    def _build_comprehensive_batch_prompt(
        self,
        all_competencies_data: List[Dict]
    ) -> str:
        """
        ì¢…í•© ë°°ì¹˜ ì¬ìƒì„± í”„ë¡¬í”„íŠ¸ (ê·¼ê±° + ê°•ì  + ì•½ì  + ê´€ì°°)
        """
        
        template = """# Task: 10ê°œ ì—­ëŸ‰ í‰ê°€ ë°ì´í„° ì¢…í•© ë°°ì¹˜ ì¬ìƒì„±

ë‹¹ì‹ ì€ HR í‰ê°€ ë³´ê³ ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
10ê°œ ì—­ëŸ‰ì— ëŒ€í•œ **ê·¼ê±°, ê°•ì , ì•½ì , í•µì‹¬ ê´€ì°°**ì„ **í•œ ë²ˆì—** í‰ì„œë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

## ì…ë ¥ ë°ì´í„°:
```json
__COMPETENCY_DATA__
```
## ğŸ”¥ CRITICAL: ì›ë³¸ Segment ì •ë³´ ì ˆëŒ€ ë³´ì¡´ ê·œì¹™

**ì ˆëŒ€ ê·œì¹™:**
- âœ… **ìµœì†Œ 2ê°œ** (ì˜ˆì™¸ ì—†ìŒ)
- âœ… **ìµœëŒ€ 8ê°œ**
- âŒ **ë¹ˆ ë°°ì—´ [] ì ˆëŒ€ ê¸ˆì§€**
- âŒ **1ê°œë§Œ ìˆëŠ” ê²ƒë„ ê¸ˆì§€**

**ëª¨ë“  evidencesëŠ” ë°˜ë“œì‹œ ì›ë³¸ ë°ì´í„°ì˜ segment_id, char_index, char_lengthë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.**

### âœ… ì˜¬ë°”ë¥¸ ë°©ë²•:
```json
// ì›ë³¸ evidence_details:
{
  "segment_id": 3,
  "char_index": 1200,
  "char_length": 45,
  "text": "..."
}

// ì¶œë ¥ evidences (ì›ë³¸ ê°’ ê·¸ëŒ€ë¡œ):
{
  "summary": "ì§€ì›ìëŠ” ...",
  "segment_id": 3,        // âœ… ì›ë³¸ ìœ ì§€
  "char_index": 1200,     // âœ… ì›ë³¸ ìœ ì§€
  "char_length": 45,      // âœ… ì›ë³¸ ìœ ì§€
  "impact": "positive"
}
```

### âŒ ì ˆëŒ€ ê¸ˆì§€:
- segment_id, char_index, char_lengthë¥¼ ì„ì˜ë¡œ ìƒì„± âŒ
- ì›ë³¸ ê°’ì„ 0ìœ¼ë¡œ ë³€ê²½ âŒ
- ê³ ì •ê°’(50, 60, 80) ë°˜ë³µ ì‚¬ìš© âŒ

### ğŸ“Œ Evidences ìƒì„± ê·œì¹™:

1. **evidence_detailsê°€ ìˆëŠ” ê²½ìš°:**
   - ê° evidence_detailì„ 1ê°œì˜ positive evidenceë¡œ ë³€í™˜
   - segment_id, char_index, char_lengthëŠ” **ì›ë³¸ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
   - summaryë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±

2. **red_flagsê°€ ìˆëŠ” ê²½ìš°:**
   - ê° red_flagë¥¼ 1ê°œì˜ negative evidenceë¡œ ë³€í™˜
   - evidence_referenceì—ì„œ segment_id, char_index íŒŒì‹±
   - char_lengthëŠ” description ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •

3. **ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°:**
   - behavioral_pattern.specific_examplesì—ì„œ segment ë²ˆí˜¸ ì°¾ê¸°
   - ê·¸ë˜ë„ ì—†ìœ¼ë©´ evidences: [] (ë¹ˆ ë°°ì—´)
```

---

### ğŸ“Œ Segment ì •ë³´ ì¶”ì¶œ ìš°ì„ ìˆœìœ„:

**1ìˆœìœ„: evidence_details**
```json
{
  "segment_id": 3,
  "char_index": 1200
}
```

**2ìˆœìœ„: behavioral_pattern.specific_examples**
```
"ëª¨ë“  ì£¼ìš” ê²½í—˜(3ê°œ)ì—ì„œ ìë°œì  ì‹œì‘ (Segment 3, 7, 11)"
â†’ segment_id: 3, 7, 11 ì¶”ì¶œ
```

**3ìˆœìœ„: behavioral_reasoning**
```
"Segment 9ì—ì„œ ëˆê¸° ëª…í™•..."
â†’ segment_id: 9 ì¶”ì¶œ
```

**4ìˆœìœ„: red_flags.evidence_reference**
```
"segment_id: 9, char_index: 3450-3500"
â†’ segment_id: 9, char_index: 3450 ì¶”ì¶œ
```

**5ìˆœìœ„: critical_reasoning**
```
"Segment 5ì—ì„œ ì™¸ì  ë™ê¸°..."
â†’ segment_id: 5 ì¶”ì¶œ
```

**ëª¨ë“  ë°©ë²•ìœ¼ë¡œë„ segmentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´:**
â†’ í•´ë‹¹ ì—­ëŸ‰ì˜ evidencesëŠ” `[]` ë¹ˆ ë°°ì—´

---

**ê²€ì¦ ê·œì¹™:**
- âœ… ëª¨ë“  evidence í•­ëª©ì˜ segment_idëŠ” 1 ì´ìƒì´ì–´ì•¼ í•¨
- âœ… char_indexëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨
- âœ… segment_idê°€ ì—†ìœ¼ë©´ í•´ë‹¹ evidence í•­ëª© ìì²´ë¥¼ ì œì™¸
- âŒ segment_id: 0, null, undefined ì ˆëŒ€ ê¸ˆì§€
---
## Rule 4: char_indexì™€ char_length ê·œì¹™

### char_index:
- **1ìˆœìœ„ evidence_detailsì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°: ì›ë³¸ ê°’ ê·¸ëŒ€ë¡œ**
- **2ìˆœìœ„ red_flagsì—ì„œ íŒŒì‹±í•œ ê²½ìš°: íŒŒì‹±ëœ ê°’ ì‚¬ìš©**
- **3~5ìˆœìœ„ë¡œ ìƒì„±í•œ ê²½ìš°: 0 ì‚¬ìš© (ì–´ì©” ìˆ˜ ì—†ìŒ)**

### char_length (í•˜ì´ë¼ì´íŠ¸ ë²”ìœ„):

**char_lengthì˜ ì˜ë¯¸:**
- í”„ë¡ íŠ¸ì—”ë“œì—ì„œ transcriptì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ í•˜ì´ë¼ì´íŠ¸í•  ë•Œ ì‚¬ìš©
- char_indexë¶€í„° char_lengthë§Œí¼ì˜ í…ìŠ¤íŠ¸ê°€ ë…¸ë€ìƒ‰ìœ¼ë¡œ í‘œì‹œë¨
- **ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´/ë¬¸ì¥ ë²”ìœ„ì—¬ì•¼ í•¨**

**ê·œì¹™:**

1. **1ìˆœìœ„: evidence_detailsì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°**
```json
   {
     "char_index": 1200,
     "char_length": 45  // âœ… ì›ë³¸ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
   }
```

2. **2ìˆœìœ„: red_flagsì—ì„œ íŒŒì‹±í•œ ê²½ìš°**
```
   evidence_reference: "segment_id: 9, char_index: 3450-3500"
```
   â†’ char_index: 3450, char_length: 50 (3500 - 3450)

3. **3~5ìˆœìœ„: behavioral/criticalì—ì„œ ìƒì„±í•œ ê²½ìš°**
   - **ì›ì¹™: transcriptì—ì„œ ì‹¤ì œë¡œ í•˜ì´ë¼ì´íŠ¸í•  í•µì‹¬ ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¶”ì •**
   - **ë°©ë²•:**
     - ì§§ì€ í•µì‹¬ í‘œí˜„ (1~2ë‹¨ì–´): 10~30ì
     - í•œ ë¬¸ì¥: 30~80ì
     - ë‘ ë¬¸ì¥: 80~150ì
   - **ì˜ˆì‹œ:**
```json
     {
       "summary": "ì§€ì›ìëŠ” 'í˜¼ì í•˜ëŠ” ê²Œ ë” í¸í•˜ë‹¤'ê³  ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤.",
       "segment_id": 8,
       "char_index": 0,
       "char_length": 40  // "í˜¼ì í•˜ëŠ” ê²Œ ë” í¸í•˜ë‹¤" ë¶€ë¶„ë§Œ í•˜ì´ë¼ì´íŠ¸
     }
```

### âœ… ì¢‹ì€ ì˜ˆì‹œ:
```json
// evidence_detailsì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°
{
  "summary": "ì§€ì›ìëŠ” êµìˆ˜ë‹˜ê»˜ ì§ì ‘ ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ì œì•ˆí•˜ì—¬ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.",
  "segment_id": 3,
  "char_index": 1200,
  "char_length": 45,  // âœ… ì›ë³¸ ê°’ (ì‹¤ì œ í•˜ì´ë¼ì´íŠ¸ ë²”ìœ„)
  "impact": "positive"
}

// red_flagsì—ì„œ íŒŒì‹±í•œ ê²½ìš°
{
  "summary": "ì§€ì›ìëŠ” 'ì–´ë ¤ìš´ ê±´ í”¼í•˜ê³  ì‰¬ìš´ ê±¸ë¡œ ì„ íƒí–ˆë‹¤'ê³  ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤.",
  "segment_id": 9,
  "char_index": 3450,
  "char_length": 50,  // âœ… 3500 - 3450 = 50 (íŒŒì‹±ëœ ë²”ìœ„)
  "impact": "negative"
}

// behavioralì—ì„œ ìƒì„±í•œ ê²½ìš°
{
  "summary": "ì§€ì›ìëŠ” í˜‘ì—…ë³´ë‹¤ ê°œì¸ ì‘ì—…ì„ ì„ í˜¸í•œë‹¤ê³  ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤.",
  "segment_id": 8,
  "char_index": 0,
  "char_length": 35,  // âœ… í•µì‹¬ ë¬¸ì¥ ê¸¸ì´ ì¶”ì • ("í˜‘ì—…ë³´ë‹¤ ê°œì¸ ì‘ì—…ì„ ì„ í˜¸í•œë‹¤")
  "impact": "neutral"
}
```

### âŒ ë‚˜ìœ ì˜ˆì‹œ:
```json
// ëª¨ë“  evidenceì—ì„œ ê³ ì •ê°’ ì‚¬ìš©
{
  "char_length": 60  // âŒ ëª¨ë“  evidenceì—ì„œ 60 ë°˜ë³µ
}

// í„°ë¬´ë‹ˆì—†ì´ ì§§ê±°ë‚˜ ê¸´ ê°’
{
  "char_length": 5   // âŒ ë„ˆë¬´ ì§§ìŒ (í•˜ì´ë¼ì´íŠ¸ ì•ˆ ë³´ì„)
}

{
  "char_length": 500  // âŒ ë„ˆë¬´ ê¹€ (ì „ì²´ ë‹µë³€ í•˜ì´ë¼ì´íŠ¸)
}
```

### ğŸ¯ char_length ê²°ì • ê°€ì´ë“œ:

**ì›ì¹™: "ì´ evidenceì˜ í•µì‹¬ì´ ë˜ëŠ” ë¶€ë¶„ì´ transcriptì—ì„œ ëª‡ ê¸€ì ì •ë„ì¼ê¹Œ?"**

1. **í•µì‹¬ í‚¤ì›Œë“œ/í‘œí˜„ë§Œ í•˜ì´ë¼ì´íŠ¸í•˜ê³  ì‹¶ìœ¼ë©´:**
   - ì˜ˆ: "í˜¼ì í•˜ëŠ” ê²Œ ë” í¸í•˜ë‹¤" â†’ 20~40ì
   
2. **í•œ ë¬¸ì¥ì„ í•˜ì´ë¼ì´íŠ¸í•˜ê³  ì‹¶ìœ¼ë©´:**
   - ì˜ˆ: "êµìˆ˜ë‹˜ê»˜ ì§ì ‘ ì œì•ˆí–ˆì–´ìš”." â†’ 30~80ì
   
3. **ë‘ ë¬¸ì¥ì„ í•˜ì´ë¼ì´íŠ¸í•˜ê³  ì‹¶ìœ¼ë©´:**
   - ì˜ˆ: "ì œê°€ ë¨¼ì € ì œì•ˆí–ˆì–´ìš”. ê¶ê¸ˆí•´ì„œ ë¬¼ì–´ë´¤ê±°ë“ ìš”." â†’ 80~150ì

**âš ï¸ ì£¼ì˜:**
- char_indexê°€ 0ì¸ ê²½ìš° (ì •í™•í•œ ìœ„ì¹˜ ëª¨ë¦„): í•µì‹¬ ë¬¸ì¥ ê¸¸ì´ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì¶”ì • (40~80ì)
- ê°™ì€ ì—­ëŸ‰ ë‚´ì—ì„œë„ evidenceë§ˆë‹¤ ë‹¤ë¥¸ ê°’ ì‚¬ìš© (ê³ ì •ê°’ ë°˜ë³µ ê¸ˆì§€)

```
## ì¶œë ¥ ìš”êµ¬ì‚¬í•­:

### 1. Evidences (ê·¼ê±°) ì¬ìƒì„±
- Evidence, Behavioral, Criticalì„ **í†µí•©í•˜ì—¬** í‰ì„œë¬¸ìœ¼ë¡œ ì¬ì‘ì„±
- ê° ê·¼ê±°ëŠ” **2-3ë¬¸ì¥** (êµ¬ì–´ì²´ âŒ, "~í–ˆìŠµë‹ˆë‹¤" âœ…)
- ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¬í•´ì„í•˜ì—¬ ì˜ë¯¸ ìˆê²Œ ì„œìˆ 

### 2. Strengths (ê°•ì ) í‰ì„œí˜• ë³€í™˜
- ì›ë³¸: "ì£¼ë„ì„± ì¤‘ìƒ (êµìˆ˜ë‹˜ê»˜ ë¨¼ì € ì œì•ˆ, ìë°œì  ì‹œì‘)"
- ë³€í™˜: "ìë°œì ì´ê³  ì£¼ë„ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
- **ê° ê°•ì ì„ 1-2ë¬¸ì¥ì˜ í‰ì„œë¬¸ìœ¼ë¡œ**

### 3. Weaknesses (ì•½ì ) í‰ì„œí˜• ë³€í™˜
- ì›ë³¸: "ìë°œì„± ë¶€ì¡± (ëª¨ë“  ê²½í—˜ì—ì„œ ìˆ˜ë™ì )"
- ë³€í™˜: "ëŒ€ë¶€ë¶„ ìƒí™©ì—ì„œ ìˆ˜ë™ì ì…ë‹ˆë‹¤."
- **ê° ì•½ì ì„ 1-2ë¬¸ì¥ì˜ í‰ì„œë¬¸ìœ¼ë¡œ**

### 4. Key_observations (í•µì‹¬ ê´€ì°°) í‰ì„œí˜• ë³€í™˜
- ì›ë³¸: "ì‹ ì… ì¹˜ê³ ëŠ” ìë°œì„±ê³¼ ë‚´ì  ë™ê¸°ê°€ ëª…í™• (ìƒìœ„ 30% ì¶”ì •)"
- ë³€í™˜: "ì‹ ì… ê¸°ì¤€ ìƒìœ„ 30% ìˆ˜ì¤€ì˜ ìë°œì„±ì„ ë³´ì…ë‹ˆë‹¤."
- **ê° ê´€ì°°ì„ 1-2ë¬¸ì¥ì˜ í‰ì„œë¬¸ìœ¼ë¡œ**
- **ìµœì†Œ 3ê°œ ì´ìƒ** (ì˜ˆì™¸ ì—†ìŒ)

### 5. ê° Evidence í•­ëª© ì •ë³´:
- **summary**: í‰ì„œë¬¸ í˜•íƒœì˜ ê·¼ê±° ì„¤ëª… (2-3ë¬¸ì¥)
- **segment_id**: transcript ìœ„ì¹˜
- **char_index**: transcript ì‹œì‘ ìœ„ì¹˜
- **char_length**: í•˜ì´ë¼ì´íŠ¸í•  í…ìŠ¤íŠ¸ ê¸¸ì´ (20~80ì)
- **impact**: "positive" / "negative" / "neutral"
- **evidence_type**: ê·¼ê±° ìœ í˜• (ì˜ˆ: "ì£¼ë„ì„±", "ëˆê¸°")

### 6. ê·¼ê±° ê°œìˆ˜ (ì—­ëŸ‰ë³„):
- Evidence details ê°œìˆ˜ë§Œí¼ positive ê·¼ê±°
- Red flags ê°œìˆ˜ë§Œí¼ negative ê·¼ê±°
- **ìµœì†Œ 3ê°œ, ìµœëŒ€ 8ê°œ**

---

## í‰ì„œë¬¸ ì‘ì„± ê°€ì´ë“œ:

âœ… **ì¢‹ì€ ì˜ˆì‹œ (ê·¼ê±°) - 2-3ë¬¸ì¥:**
"ì§€ì›ìëŠ” êµìˆ˜ë‹˜ê»˜ ì§ì ‘ ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ì œì•ˆí•˜ì—¬ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë†’ì€ ì£¼ë„ì„±ê³¼ ë‚´ì  ë™ê¸°ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ë¡œ, ìŠ¤ìŠ¤ë¡œ 'ê¶ê¸ˆí•´ì„œ' ë¨¼ì € ë¬¼ì–´ë³´ê³  í–‰ë™ì— ì˜®ê¸´ ì ì´ ì¸ìƒì ì…ë‹ˆë‹¤."

âœ… **ì¢‹ì€ ì˜ˆì‹œ (ê°•ì ) - 1ë¬¸ì¥, ê°„ê²°:**
"ìë°œì ì´ê³  ì£¼ë„ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
"ë„ì „ì ì¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³  ëê¹Œì§€ ì™„ìˆ˜í•©ë‹ˆë‹¤."
"ì‹¤íŒ¨ í›„ì—ë„ ì¬ë„ì „í•˜ëŠ” ëˆê¸°ê°€ ìˆìŠµë‹ˆë‹¤."

âœ… **ì¢‹ì€ ì˜ˆì‹œ (ì•½ì ) - 1ë¬¸ì¥, ê°„ê²°:**
"ì¼ë¶€ ìƒí™©ì—ì„œ ë„ì „ì„ íšŒí”¼í•©ë‹ˆë‹¤."
"í”„ë¡œì íŠ¸ ì™„ìˆ˜ìœ¨ì´ ë‹¤ì†Œ ë‚®ìŠµë‹ˆë‹¤."
"ì™¸ì  ë™ê¸°ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤."

âœ… **ì¢‹ì€ ì˜ˆì‹œ (ê´€ì°°) - 1ë¬¸ì¥, ê°„ê²°:**
"ì‹ ì… ê¸°ì¤€ ìƒìœ„ 30% ìˆ˜ì¤€ì˜ ìë°œì„±ì„ ë³´ì…ë‹ˆë‹¤."
"ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì¼ê´€ëœ ì£¼ë„ì„±ì„ ë³´ì…ë‹ˆë‹¤."
"ì…ì‚¬ í›„ ì–´ë ¤ìš´ ê³¼ì œì—ë„ í¬ê¸°í•˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤."

âŒ **ë‚˜ìœ ì˜ˆì‹œ (ê°•ì /ì•½ì /ê´€ì°°) - ë„ˆë¬´ ê¸¸ê³  ì¥í™©í•¨:**
"ì§€ì›ìëŠ” êµìˆ˜ë‹˜ê»˜ ë¨¼ì € ì œì•ˆí•˜ë©° ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ìë°œì ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë“± ë†’ì€ ì£¼ë„ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤." (X)
"ë™ì•„ë¦¬ í™œë™ ì„ íƒ ì‹œ ì–´ë ¤ìš´ ê³¼ì œë¥¼ íšŒí”¼í•˜ê³  ì‰¬ìš´ ì˜µì…˜ì„ ì„ íƒí•˜ëŠ” ê²½í–¥ì´ ì¼ë¶€ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤." (X)
"ì‹ ì… ì§€ì›ìë¡œì„œëŠ” ë“œë¬¼ê²Œ ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì¼ê´€ëœ ìë°œì  ì‹œì‘ íŒ¨í„´ì„ ë³´ì˜€ìœ¼ë©°, ì´ëŠ” ë™ì¼ ì§ê¸‰ ëŒ€ë¹„ ìƒìœ„ 30% ìˆ˜ì¤€ì˜ ì£¼ë„ì„±ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤." (X)


---

## ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "competencies": [
    {{
      "competency_name": "achievement_motivation",
      
      "evidences": [
        {{
          "summary": "ì§€ì›ìëŠ” êµìˆ˜ë‹˜ê»˜ ì§ì ‘ ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ì œì•ˆí•˜ì—¬ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë†’ì€ ì£¼ë„ì„±ê³¼ ë‚´ì  ë™ê¸°ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ë¡œ, ìŠ¤ìŠ¤ë¡œ 'ê¶ê¸ˆí•´ì„œ' ë¨¼ì € ë¬¼ì–´ë³´ê³  í–‰ë™ì— ì˜®ê¸´ ì ì´ ì¸ìƒì ì…ë‹ˆë‹¤.",
          "segment_id": 3,
          "char_index": 1200,
          "char_length": 45,
          "impact": "positive",
          "evidence_type": "ì£¼ë„ì„±, ë‚´ì  ë™ê¸°"
        }},
        {{
          "summary": "ì§€ì›ìëŠ” ë™ì•„ë¦¬ í™œë™ ì„ íƒ ì‹œ 'ì–´ë ¤ìš´ ê±´ í”¼í•˜ê³  ì‰¬ìš´ ê±¸ë¡œ ì„ íƒí–ˆë‹¤'ê³  ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¼ë¶€ ìƒí™©ì—ì„œ ë„ì „ì„ íšŒí”¼í•˜ëŠ” ê²½í–¥ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.",
          "segment_id": 9,
          "char_index": 3450,
          "char_length": 30,
          "impact": "negative",
          "evidence_type": "ë„ì „ íšŒí”¼"
        }}
      ],
      
      "strengths": [
        "ìë°œì ì´ê³  ì£¼ë„ì„±ì´ ë†’ìŠµë‹ˆë‹¤.",
        "ë„ì „ì ì¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³  ëê¹Œì§€ ì™„ìˆ˜í•©ë‹ˆë‹¤.",
        "ë‚´ì  ë™ê¸°ê°€ ëª…í™•í•©ë‹ˆë‹¤.",
        "ì‹¤íŒ¨ í›„ì—ë„ ì¬ë„ì „í•˜ëŠ” ëˆê¸°ê°€ ìˆìŠµë‹ˆë‹¤."
      ],
      
      "weaknesses": [
        "ì¼ë¶€ ìƒí™©ì—ì„œ ë„ì „ì„ íšŒí”¼í•©ë‹ˆë‹¤.",
        "í”„ë¡œì íŠ¸ ì™„ìˆ˜ìœ¨ì´ ë‹¤ì†Œ ë‚®ìŠµë‹ˆë‹¤."
      ],
      
      "key_observations": [
        "ì‹ ì… ê¸°ì¤€ ìƒìœ„ 30% ìˆ˜ì¤€ì˜ ìë°œì„±ì„ ë³´ì…ë‹ˆë‹¤.",
        "ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì¼ê´€ëœ ì£¼ë„ì„±ì„ ë³´ì…ë‹ˆë‹¤.",
        "ì…ì‚¬ í›„ ì–´ë ¤ìš´ ê³¼ì œì—ë„ í¬ê¸°í•˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤."
      ]
    }},
    {{
      "competency_name": "growth_potential",
      "evidences": [...],
      "strengths": [...],
      "weaknesses": [...],
      "key_observations": [...]
    }},
    ... (ì´ 10ê°œ ì—­ëŸ‰)
  ]
}}

---

**ì¤‘ìš”:**
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥ (ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ âŒ)
- 10ê°œ ì—­ëŸ‰ ëª¨ë‘ í¬í•¨
- **EvidencesëŠ” 2-3ë¬¸ì¥ (ìƒì„¸)**
- **Strengths/Weaknesses/Key_observationsëŠ” 1ë¬¸ì¥ (ê°„ê²°)**
- ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , **ì˜ë¯¸ë¥¼ ì¬í•´ì„**
- StrengthsëŠ” ìµœì†Œ 3ê°œ, ìµœëŒ€ 6ê°œ
- WeaknessesëŠ” ìµœì†Œ 2ê°œ, ìµœëŒ€ 4ê°œ
- Key_observationsëŠ” ìµœì†Œ 3ê°œ, ìµœëŒ€ 5ê°œ
"""
        
        prompt = template.replace(
            "__COMPETENCY_DATA__",
            json.dumps(all_competencies_data, ensure_ascii=False, indent=2)
        )

        return prompt

    def _get_transcript_text(self, segment_id: int, char_index: int) -> str:
        """
        í…ŒìŠ¤íŠ¸ìš© transcriptì—ì„œ segment ë‹µë³€ í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ë°˜í™˜.
        - ê²½ë¡œ ìš°ì„ ìˆœìœ„: tests_data/ â†’ server/test_data/ â†’ test_data/
        - char_indexê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ìœ„ì¹˜ì—ì„œ 120ì ìŠ¬ë¼ì´ìŠ¤, ì—†ìœ¼ë©´ ì „ì²´ ë‹µë³€ ë°˜í™˜.
        """
        
        segments = self._transcript_data.get("segments", []) if isinstance(self._transcript_data, dict) else []
        segment = next((s for s in segments if s.get("segment_id") == segment_id), None)
        if not segment:
            return ""
        
        answer_text = segment.get("answer_text", "")
        if not isinstance(answer_text, str):
            return ""
        
        if isinstance(char_index, int) and 0 <= char_index < len(answer_text):
            return answer_text[char_index: char_index + 120]
        
        return answer_text
    
    
    def _fallback_all_batch(
        self,
        aggregated_competencies: Dict
    ) -> Dict[str, Dict]:
        """
        ë°°ì¹˜ LLM ì‹¤íŒ¨ ì‹œ Fallback (ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ)
        """
        
        fallback = {}
        
        for comp_name, comp_data in aggregated_competencies.items():
            perspectives = comp_data.get("perspectives", {})
            evidence_details = perspectives.get("evidence_details", [])
            red_flags = perspectives.get("red_flags", [])
            comp_evidences = []
            
            # Evidence details
            for ev in evidence_details:
                comp_evidences.append({
                    "summary": f"ì§€ì›ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ í–‰ë™ì„ ë³´ì˜€ìŠµë‹ˆë‹¤: {ev.get('text', '')}",
                    "segment_id": ev.get("segment_id"),
                    "char_index": ev.get("char_index"),
                    "char_length": 50,
                    "impact": "positive",
                    "evidence_type": ev.get("relevance_note", "")
                })
            
            # Red flags
            for flag in red_flags:
                seg_id = self._extract_segment_id(flag.get("evidence_reference", ""))
                char_idx = self._extract_char_index(flag.get("evidence_reference", ""))
                
                comp_evidences.append({
                    "summary": flag.get("description", ""),
                    "segment_id": seg_id,
                    "char_index": char_idx,
                    "char_length": 30,
                    "impact": "negative",
                    "evidence_type": flag.get("flag_type", "")
                })
            
            fallback[comp_name] = {
                "evidences": comp_evidences,
                "strengths": comp_data.get("strengths", []),
                "weaknesses": comp_data.get("weaknesses", []),
                "key_observations": comp_data.get("key_observations", [])
            }
        
        return fallback
    
    
    @staticmethod
    def _extract_segment_id(evidence_reference: str) -> int:
        """evidence_referenceì—ì„œ segment_id ì¶”ì¶œ"""
        try:
            if "segment_id:" in evidence_reference:
                segment_part = evidence_reference.split("segment_id:")[1].split(",")[0].strip()
                return int(segment_part)
        except:
            pass
        return None
    
    
    @staticmethod
    def _extract_char_index(evidence_reference: str) -> int:
        """evidence_referenceì—ì„œ char_index ì¶”ì¶œ"""
        try:
            if "char_index:" in evidence_reference:
                char_part = evidence_reference.split("char_index:")[1].split("-")[0].strip()
                return int(char_part)
        except:
            pass
        return None


async def presentation_formatter_node(state) -> Dict:
    """
    Presentation Formatter Node
    
    í”„ë¡ íŠ¸ì—”ë“œìš© ë°ì´í„° ì¬êµ¬ì„± + ë°°ì¹˜ LLM ê·¼ê±° ì¬ìƒì„±
    """
    
    start_time = datetime.now()
    
    print("\n" + "="*60)
    print("[Presentation Formatter] í”„ë¡ íŠ¸ìš© ë°ì´í„° ë³€í™˜ ì‹œì‘")
    print("="*60)
    
    openai_client = state.get("openai_client")
    final_result = state.get("final_result", {})
    aggregated_competencies = state.get("aggregated_competencies", {})
    competency_weights = state.get("competency_weights", {})
    transcript = state.get("transcript")
    formatter = PresentationFormatter(openai_client)
    
    presentation_result = await formatter.format(
        final_result,
        aggregated_competencies,
        competency_weights,
        transcript
    )
    
    duration = (datetime.now() - start_time).total_seconds()
    
    total_evidences = sum(
        len(cd.get('evidences', [])) 
        for cd in presentation_result['competency_details'].values()
    )
    
    print(f"\n  ì´ ê·¼ê±° ìƒì„±: {total_evidences}ê°œ")
    print(f"  ê°•ì /ì•½ì /ê´€ì°° í‰ì„œí˜• ë³€í™˜: 10ê°œ ì—­ëŸ‰")
    print(f"  Resume ê²€ì¦ ê²°ê³¼ í¬í•¨: ì™„ë£Œ")
    print(f"  ì²˜ë¦¬ ì‹œê°„: {duration:.2f}ì´ˆ")
    print(f"  ë°°ì¹˜ íš¨ìœ¨: 10ê°œ ì—­ëŸ‰ â†’ 1íšŒ LLM í˜¸ì¶œ")
    
    print("\n" + "="*60)
    print("[Presentation Formatter] ì™„ë£Œ")
    print("="*60)
    
    return {
        "presentation_result": presentation_result,
        "execution_logs": state.get("execution_logs", []) + [{
            "node": "presentation_formatter",
            "duration_seconds": round(duration, 2),
            "total_evidences_generated": total_evidences,
            "batch_llm_calls": 1,
            "components_regenerated": ["evidences", "strengths", "weaknesses", "key_observations"],
            "timestamp": datetime.now().isoformat()
        }]
    }
