"""
Final Integrator
Stage 3: ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì‹ ë¢°ë„ í‰ê°€

ì²˜ë¦¬ ë‚´ìš©:
    1. 10ê°œ ì—­ëŸ‰ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
    2. í‰ê·  Confidence V2 ê³„ì‚°
    3. ì‹ ë¢°ë„ ë ˆë²¨ íŒë‹¨
    4. ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± (AI í˜¸ì¶œ)
    5. ìµœì¢… ë¦¬í¬íŠ¸ êµ¬ì„±
"""

import json
from typing import Dict, List, Optional
from datetime import datetime
from openai import AsyncOpenAI

from .state import EvaluationState
from services.evaluation.post_processing_service import PostProcessingService


class FinalIntegrator:
    """
    ìµœì¢… í†µí•©ê¸°
    
    ì—­í• :
        - Job/Common êµ¬ë¶„ ì—†ì´ 10ê°œ ì—­ëŸ‰ ì§ì ‘ ì²˜ë¦¬
        - Confidence V2 ê¸°ë°˜ ì‹ ë¢°ë„ í‰ê°€
        - Collaboration ê²°ê³¼ ë°˜ì˜
        - ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± (AI)
    """
    
    # ì‹ ë¢°ë„ ë ˆë²¨ Threshold
    RELIABILITY_THRESHOLDS = {
        "very_high": 0.85,  # ë§¤ìš° ë†’ìŒ
        "high": 0.70,       # ë†’ìŒ
        "medium": 0.55,     # ì¤‘ê°„
        "low": 0.0          # ë‚®ìŒ
    }
    
    
    @staticmethod
    async def integrate(
        openai_client: AsyncOpenAI,
        aggregated_competencies: Dict[str, Dict],
        competency_weights: Dict[str, float],
        collaboration_results: Optional[List[Dict]] = None,
        low_confidence_list: Optional[List[Dict]] = None
    ) -> Dict:
        """
        ìµœì¢… í†µí•©
        
        Args:
            openai_client: OpenAI AsyncClient (ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„±ìš©)
            aggregated_competencies: Aggregatorì—ì„œ ì§‘ê³„ëœ 10ê°œ ì—­ëŸ‰
                {
                    "achievement_motivation": {
                        "overall_score": 85,
                        "confidence_v2": 0.85,
                        "key_observations": [...],
                        "resume_verification_summary": {...},
                        ...
                    },
                    ...
                }
            
            competency_weights: 10ê°œ ì—­ëŸ‰ ê°€ì¤‘ì¹˜
                {
                    "achievement_motivation": 0.12,
                    "growth_potential": 0.10,
                    ...
                }
            
            collaboration_results: Collaboration Node ê²°ê³¼ (ì„ íƒì )
            low_confidence_list: Low Confidence ëª©ë¡
        
        Returns:
            final_result: ìµœì¢… ë¦¬í¬íŠ¸
                {
                    "final_score": 82.5,
                    "avg_confidence": 0.78,
                    "reliability": {...},
                    "overall_evaluation_summary": "...",  # ğŸ†• ì¢…í•© ì‹¬ì‚¬í‰
                    "competency_scores": [...],
                    "competency_details": {...},  # Resume ê²€ì¦ ê·¼ê±° í¬í•¨
                    "collaboration_summary": {...},
                    "low_confidence_summary": {...}
                }
        """
        
        print("\n[Final Integrator] ìµœì¢… í†µí•© ì‹œì‘")
        
    
        # 1. Collaboration ê²°ê³¼ ë°˜ì˜       
        if collaboration_results:
            print(f"  Collaboration ê²°ê³¼ ë°˜ì˜: {len(collaboration_results)}ê±´")
            aggregated_competencies = FinalIntegrator._apply_collaboration_results(
                aggregated_competencies,
                collaboration_results
            )
        else:
            print("  Collaboration ê²°ê³¼ ì—†ìŒ (ìŠ¤í‚µ)")
        
        
    
        # 2. ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· ) 
        final_score, competency_scores = FinalIntegrator._calculate_final_score(
            aggregated_competencies,
            competency_weights
        )
        
        print(f"\n  ìµœì¢… ì ìˆ˜: {final_score:.1f}ì ")
        
        
    
        # 3. í‰ê·  Confidence V2 ê³„ì‚°
        avg_confidence = FinalIntegrator._calculate_avg_confidence(
            aggregated_competencies,
            competency_weights
        )
        
        print(f"  í‰ê·  Confidence V2: {avg_confidence:.2f}")
        
        
    
        # 4. ì‹ ë¢°ë„ ë ˆë²¨ íŒë‹¨
        reliability = FinalIntegrator._determine_reliability(
            avg_confidence,
            low_confidence_list or []
        )
        
        print(f"  ì‹ ë¢°ë„ ë ˆë²¨: {reliability['level']}")
        print(f"  ì‹ ë¢°ë„ ê·¼ê±°: {reliability['note']}")
        
        
    
        # 5. ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± (AI í˜¸ì¶œ)
        print("\n   ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± ì¤‘ (AI í˜¸ì¶œ)...")
        overall_evaluation_summary = await FinalIntegrator._generate_overall_evaluation_summary(
            openai_client,
            aggregated_competencies,
            final_score,
            avg_confidence,
            reliability
        )
        print(f"   ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± ì™„ë£Œ ({len(overall_evaluation_summary)} chars)")
        
        
    
        # 6. Collaboration ìš”ì•½
        collaboration_summary = FinalIntegrator._summarize_collaboration(
            collaboration_results or []
        )
        
        
    
        # 7. Low Confidence ìš”ì•½
        low_confidence_summary = FinalIntegrator._summarize_low_confidence(
            low_confidence_list or []
        )
        
        
    
        # 8. ìµœì¢… ë¦¬í¬íŠ¸ êµ¬ì„±
        final_result = {
            "final_score": final_score,
            "avg_confidence": avg_confidence,
            
            "reliability": reliability,
            
            #  ì¢…í•© ì‹¬ì‚¬í‰ (AI ìƒì„±)
            "overall_evaluation_summary": overall_evaluation_summary,
            
            "competency_scores": competency_scores,
            
            "collaboration_summary": collaboration_summary,
            
            "low_confidence_summary": low_confidence_summary,
            
            # ì—­ëŸ‰ë³„ ìƒì„¸ (Resume ê²€ì¦ ê·¼ê±° í¬í•¨)
            "competency_details": {
                comp_name: {
                    "overall_score": comp_data["overall_score"],
                    "confidence_v2": comp_data["confidence_v2"],
                    "weight": competency_weights.get(comp_name, 0.0),
                    "weighted_contribution": comp_data["overall_score"] * competency_weights.get(comp_name, 0.0),
                    "resume_verified_count": comp_data.get("resume_verified_count", 0),
                    "segment_count": comp_data.get("segment_count", 0),
                    "strengths": comp_data.get("strengths", []),
                    "weaknesses": comp_data.get("weaknesses", []),
                    "key_observations": comp_data.get("key_observations", []),
                    "resume_verification_summary": comp_data.get("resume_verification_summary", {})  # ğŸ†•
                }
                for comp_name, comp_data in aggregated_competencies.items()
            }
        }
        
        print("\n[Final Integrator] ìµœì¢… í†µí•© ì™„ë£Œ")
        
        return final_result
    
    
    @staticmethod
    async def _generate_overall_evaluation_summary(
        openai_client: AsyncOpenAI,
        aggregated_competencies: Dict[str, Dict],
        final_score: float,
        avg_confidence: float,
        reliability: Dict
    ) -> str:
        """
        ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± (AI í˜¸ì¶œ)
        
        ì „ëµ: Option 1 - ì—­ëŸ‰ë³„ í•µì‹¬ ê´€ì°° (key_observations)ì„ ì¢…í•©í•˜ì—¬ ì‹¬ì‚¬í‰ ì‘ì„±
        
        Returns:
            5-7ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ì¢…í•© ì‹¬ì‚¬í‰
        """
        
        # ì—­ëŸ‰ë³„ í•µì‹¬ ê´€ì°° ë° ì ìˆ˜ ì •ë¦¬
        competency_summary = {}
        total_resume_verified = 0
        
        for comp_name, comp_data in aggregated_competencies.items():
            competency_summary[comp_name] = {
                "score": comp_data["overall_score"],
                "confidence_v2": comp_data["confidence_v2"],
                "key_observations": comp_data.get("key_observations", []),
                "strengths": comp_data.get("strengths", []),
                "weaknesses": comp_data.get("weaknesses", []),
                "resume_verified_count": comp_data.get("resume_verified_count", 0)
            }
            total_resume_verified += comp_data.get("resume_verified_count", 0)
        
        # AI í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¹ì‹ ì€ íŒ¨ì…˜ MD ì§ë¬´ ì±„ìš©ì„ ìœ„í•œ HR ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

10ê°œ ì—­ëŸ‰ í‰ê°€ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì§€ì›ìì— ëŒ€í•œ **ìµœì¢… ì‹¬ì‚¬í‰**ì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

## ì—­ëŸ‰ë³„ í‰ê°€ ìš”ì•½:
```json
{json.dumps(competency_summary, ensure_ascii=False, indent=2)}
```

## ìµœì¢… ì ìˆ˜: {final_score:.1f}ì  / 100ì 
## í‰ê·  ì‹ ë¢°ë„ (Confidence V2): {avg_confidence:.2f}
## ì‹ ë¢°ë„ ë ˆë²¨: {reliability['level_display']}
## Resume ê²€ì¦: ì´ {total_resume_verified}ê°œ ì¦ê±° í™•ì¸ë¨

---

## ì¢…í•© ì‹¬ì‚¬í‰ ì‘ì„± ê°€ì´ë“œë¼ì¸:

### 1. í‰ê°€ì˜ ì´ˆì 
- **ì¢…í•©ì  í‰ê°€**: ê°•ì /ì•½ì  ë‚˜ì—´ì´ ì•„ë‹Œ, ì§€ì›ìì˜ ì „ì²´ì ì¸ ëª¨ìŠµì„ ì…ì²´ì ìœ¼ë¡œ í‰ê°€
- **ì§ë¬´ ì í•©ì„±**: íŒ¨ì…˜ MD ì§ë¬´ì— ì–¼ë§ˆë‚˜ ì í•©í•œì§€
- **ì—­ëŸ‰ ê°„ ê· í˜•**: 10ê°œ ì—­ëŸ‰ì´ ê³ ë¥´ê²Œ ë°œë‹¬í–ˆëŠ”ì§€, í¸ì¤‘ë˜ì§€ ì•Šì•˜ëŠ”ì§€
- **Resume ì‹ ë¢°ë„**: ë©´ì ‘ ë‹µë³€ì´ Resume ê²½ë ¥ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€
- **ì„±ì¥ ê°€ëŠ¥ì„±**: ì‹ ì… ê¸°ì¤€ìœ¼ë¡œ í–¥í›„ ì„±ì¥ ì ì¬ë ¥ì´ ìˆëŠ”ì§€

### 2. ì‹¬ì‚¬í‰ êµ¬ì¡° (5-7ë¬¸ì¥)
1ë¬¸ì¥: ì „ì²´ì ì¸ ì¸ìƒ ë° ì¢…í•© í‰ê°€
2-3ë¬¸ì¥: ì£¼ìš” ê°•ì  (ë†’ì€ ì ìˆ˜ ì—­ëŸ‰ ì¤‘ì‹¬, Resume ê²€ì¦ ì–¸ê¸‰)
1-2ë¬¸ì¥: ë³´ì™„ í•„ìš” ì˜ì—­ (ë‚®ì€ ì ìˆ˜ ì—­ëŸ‰, ê· í˜• ë¬¸ì œ)
1ë¬¸ì¥: ìµœì¢… íŒë‹¨ (ì±„ìš© ì¶”ì²œ ì—¬ë¶€, ì„±ì¥ ê¸°ëŒ€)

### 3. ì£¼ì˜ì‚¬í•­
- âŒ "ì„±ì·¨ë™ê¸°ê°€ ë†’ìŠµë‹ˆë‹¤. ì„±ì¥ì ì¬ë ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤." ê°™ì€ ë‹¨ìˆœ ë‚˜ì—´ ê¸ˆì§€
- âœ… "ì§€ì›ìëŠ” ë†’ì€ ì„±ì·¨ë™ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìê¸°ì£¼ë„ì  í•™ìŠµì„ í•´ì™”ìœ¼ë©°..." ê°™ì€ ì—°ê²°ëœ ì„œìˆ 
- âŒ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì–¸ê¸‰ ("85ì ", "70ì ") ê¸ˆì§€
- âœ… "ìš°ìˆ˜í•¨", "ì–‘í˜¸í•¨", "ë³´ì™„ í•„ìš”" ê°™ì€ í‘œí˜„ ì‚¬ìš©
- âœ… Resume ê²€ì¦ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì–¸ê¸‰
  - ì˜ˆ: "Resumeì—ì„œë„ ë¦¬í…Œì¼ ë¶„ì„ ê³µëª¨ì „ ì…ìƒ ê²½ë ¥ì´ í™•ì¸ë˜ì–´ ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤"

### 4. ì‹ ì… ê¸°ì¤€ í‰ê°€
- ì‹ ì… ì§€ì›ìì„ì„ ì—¼ë‘ì— ë‘ê³  í‰ê°€
- "ê²½í—˜ ë¶€ì¡±"ë³´ë‹¤ëŠ” "ì„±ì¥ ê°€ëŠ¥ì„±", "í•™ìŠµ íƒœë„" ì¤‘ì‹¬
- ê³¼ë„í•œ ê¸°ëŒ€ì¹˜ ì ìš© ê¸ˆì§€

---

## ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "overall_evaluation_summary": "ì§€ì›ìëŠ” íŒ¨ì…˜ MD ì§ë¬´ì— í•„ìš”í•œ ëŒ€ë¶€ë¶„ì˜ ì—­ëŸ‰ì„ ê³ ë¥´ê²Œ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê³ ê° ì—¬ì • ì„¤ê³„ ë° ë§ˆì¼€íŒ… í†µí•© ì „ëµ ì—­ëŸ‰ì´ ìš°ìˆ˜í•˜ë©°, Resumeì—ì„œë„ ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ë‹¤ìˆ˜ í™•ì¸ë˜ì–´ ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ë° ìƒí’ˆ ê¸°íš ì—­ëŸ‰ë„ ì–‘í˜¸í•˜ì—¬ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì´ ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë‹¤ë§Œ ì¡°ì§ ì í•©ì„±ê³¼ ìœ ê´€ë¶€ì„œ í˜‘ì—… ì—­ëŸ‰ì´ ë‹¤ì†Œ ë‚®ì•„, íŒ€ ë¬¸í™” ì ì‘ ë° ì´í•´ê´€ê³„ì ì¡°ì •ì— ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ì‹ ì… ê¸°ì¤€ ìƒìœ„ê¶Œ ìˆ˜ì¤€ì´ë©°, ì…ì‚¬ í›„ ì²´ê³„ì ì¸ êµìœ¡ê³¼ ë©˜í† ë§ì„ í†µí•´ ë¹ ë¥¸ ì„±ì¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤."
}}

---

**ì¤‘ìš”**: 
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.
- ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ (```) ì‚¬ìš© ê¸ˆì§€.
- overall_evaluation_summaryëŠ” í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„± (ì¤„ë°”ê¿ˆ ì—†ìŒ).
"""
        
        try:
            response = await openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert HR evaluator specializing in fashion MD hiring. Create comprehensive, insightful evaluation summaries."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # ë§ˆí¬ë‹¤ìš´ ì œê±°
            if result_text.startswith("```"):
                result_text = result_text.split("```")[1]
                if result_text.startswith("json"):
                    result_text = result_text[4:]
            result_text = result_text.strip()
            
            result = json.loads(result_text)
            return result.get("overall_evaluation_summary", "")
        
        except Exception as e:
            print(f"    âš ï¸  ì¢…í•© ì‹¬ì‚¬í‰ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return f"ì§€ì›ìëŠ” íŒ¨ì…˜ MD ì§ë¬´ì— í•„ìš”í•œ ì—­ëŸ‰ì„ ì „ë°˜ì ìœ¼ë¡œ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤ (ì¢…í•© ì ìˆ˜: {final_score:.1f}ì ). ì‹ ì… ê¸°ì¤€ìœ¼ë¡œ ì í•©í•˜ë©°, ì…ì‚¬ í›„ ì„±ì¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤."
    
    
    @staticmethod
    def _apply_collaboration_results(
        aggregated_competencies: Dict[str, Dict],
        collaboration_results: List[Dict]
    ) -> Dict[str, Dict]:
        """
        Collaboration ê²°ê³¼ë¥¼ ì—­ëŸ‰ ì ìˆ˜ì— ë°˜ì˜
        
        Collaboration ê²°ê³¼ ì˜ˆì‹œ:
            [
                {
                    "competency": "achievement_motivation",
                    "original_score": 85,
                    "adjusted_score": 88,
                    "confidence_adjusted": 0.80,
                    "reason": "Adversarial validation"
                },
                ...
            ]
        """
        
        updated = aggregated_competencies.copy()
        
        for collab in collaboration_results:
            comp_name = collab.get("competency")
            
            if comp_name not in updated:
                continue
            
            # ì ìˆ˜ ë° Confidence ì¡°ì •
            updated[comp_name] = {
                **updated[comp_name],
                "overall_score": collab.get("adjusted_score", updated[comp_name]["overall_score"]),
                "confidence_v2": collab.get("confidence_adjusted", updated[comp_name]["confidence_v2"]),
                "collaboration_applied": True,
                "collaboration_reason": collab.get("reason")
            }
        
        return updated
    
    
    @staticmethod
    def _calculate_final_score(
        aggregated_competencies: Dict[str, Dict],
        competency_weights: Dict[str, float]
    ) -> tuple[float, List[Dict]]:
        """
        ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
        
        Returns:
            (final_score, competency_scores)
            
            competency_scores: ì—­ëŸ‰ë³„ ê¸°ì—¬ë„
                [
                    {
                        "competency": "achievement_motivation",
                        "score": 85,
                        "weight": 0.12,
                        "contribution": 10.2
                    },
                    ...
                ]
        """
        
        total_weighted_score = 0.0
        total_weight = 0.0
        competency_scores = []
        
        for comp_name, comp_data in aggregated_competencies.items():
            score = comp_data["overall_score"]
            weight = competency_weights.get(comp_name, 0.0)
            
            contribution = score * weight
            
            total_weighted_score += contribution
            total_weight += weight
            
            competency_scores.append({
                "competency": comp_name,
                "score": score,
                "weight": weight,
                "contribution": round(contribution, 2),
                "confidence_v2": comp_data["confidence_v2"]
            })
        
        # ê°€ì¤‘ì¹˜ í•©ì´ 1.0ì´ ì•„ë‹ ê²½ìš° ì •ê·œí™”
        if total_weight > 0:
            final_score = total_weighted_score / total_weight
        else:
            final_score = 0.0
        
        # ì •ë ¬ (ê¸°ì—¬ë„ ë†’ì€ ìˆœ)
        competency_scores.sort(key=lambda x: x["contribution"], reverse=True)
        
        return round(final_score, 1), competency_scores
    
    
    @staticmethod
    def _calculate_avg_confidence(
        aggregated_competencies: Dict[str, Dict],
        competency_weights: Dict[str, float]
    ) -> float:
        """
        ê°€ì¤‘ í‰ê·  Confidence V2 ê³„ì‚°
        """
        
        total_weighted_conf = 0.0
        total_weight = 0.0
        
        for comp_name, comp_data in aggregated_competencies.items():
            conf_v2 = comp_data["confidence_v2"]
            weight = competency_weights.get(comp_name, 0.0)
            
            total_weighted_conf += conf_v2 * weight
            total_weight += weight
        
        if total_weight > 0:
            avg_conf = total_weighted_conf / total_weight
        else:
            avg_conf = 0.5
        
        return round(avg_conf, 2)
    
    
    @staticmethod
    def _determine_reliability(
        avg_confidence: float,
        low_confidence_list: List[Dict]
    ) -> Dict:
        """
        ì‹ ë¢°ë„ ë ˆë²¨ íŒë‹¨
        
        Returns:
            {
                "level": "high",  # "very_high", "high", "medium", "low"
                "level_display": "ë†’ìŒ",
                "note": "í‰ê·  Confidence V2ê°€ ë†’ê³  (0.78), Low Confidence ì—­ëŸ‰ ì—†ìŒ"
            }
        """
        
        # ë ˆë²¨ ê²°ì •
        if avg_confidence >= FinalIntegrator.RELIABILITY_THRESHOLDS["very_high"]:
            level = "very_high"
            level_kr = "ë§¤ìš° ë†’ìŒ"
        elif avg_confidence >= FinalIntegrator.RELIABILITY_THRESHOLDS["high"]:
            level = "high"
            level_kr = "ë†’ìŒ"
        elif avg_confidence >= FinalIntegrator.RELIABILITY_THRESHOLDS["medium"]:
            level = "medium"
            level_kr = "ì¤‘ê°„"
        else:
            level = "low"
            level_kr = "ë‚®ìŒ"
        
        # ê·¼ê±° ì„¤ëª…
        note_parts = [
            f"í‰ê·  Confidence V2: {avg_confidence:.2f}"
        ]
        
        if low_confidence_list:
            note_parts.append(f"Low Confidence ì—­ëŸ‰: {len(low_confidence_list)}ê°œ")
            for item in low_confidence_list:
                note_parts.append(f"  - {item['competency']}: {item['confidence_v2']:.2f}")
        else:
            note_parts.append("ëª¨ë“  ì—­ëŸ‰ì´ ì‹ ë¢°ë„ ê¸°ì¤€ ì¶©ì¡±")
        
        note = " | ".join(note_parts)
        
        return {
            "level": level,
            "level_display": level_kr,
            "avg_confidence": avg_confidence,
            "note": note
        }
    
    
    @staticmethod
    def _summarize_collaboration(
        collaboration_results: List[Dict]
    ) -> Dict:
        """
        Collaboration ìš”ì•½
        
        Returns:
            {
                "total_collaborations": 3,
                "competencies_adjusted": ["achievement_motivation", ...],
                "adjustments": [...]
            }
        """
        
        if not collaboration_results:
            return {
                "total_collaborations": 0,
                "competencies_adjusted": [],
                "adjustments": []
            }
        
        competencies_adjusted = [c["competency"] for c in collaboration_results]
        
        return {
            "total_collaborations": len(collaboration_results),
            "competencies_adjusted": competencies_adjusted,
            "adjustments": collaboration_results
        }
    
    
    @staticmethod
    def _summarize_low_confidence(
        low_confidence_list: List[Dict]
    ) -> Dict:
        """
        Low Confidence ìš”ì•½
        
        Returns:
            {
                "total_low_confidence": 2,
                "competencies": ["growth_potential", "organizational_fit"],
                "details": [...]
            }
        """
        
        if not low_confidence_list:
            return {
                "total_low_confidence": 0,
                "competencies": [],
                "details": []
            }
        
        competencies = [item["competency"] for item in low_confidence_list]
        
        return {
            "total_low_confidence": len(low_confidence_list),
            "competencies": competencies,
            "details": low_confidence_list
        }


async def final_integration_node(state: EvaluationState) -> Dict:
    """
    Stage 3: Final Integration Node
    
    Aggregator ê²°ê³¼ì™€ (ì„ íƒì ) Collaboration ê²°ê³¼ë¥¼ ìµœì¢… ë¦¬í¬íŠ¸ë¡œ í†µí•©í•©ë‹ˆë‹¤.
    """
    start_time = datetime.now()

    print("\n" + "=" * 60)
    print("[Stage 3] Final Integration ì‹œì‘")
    print("=" * 60)

    aggregated_competencies = state.get("aggregated_competencies", {})
    competency_weights = state.get("competency_weights", {})
    collaboration_results = state.get("collaboration_results", [])
    low_confidence_list = state.get("low_confidence_list", [])
    openai_client = state.get("openai_client")
    post_processing_service = PostProcessingService()

    final_result = await FinalIntegrator.integrate(
        openai_client=openai_client,
        aggregated_competencies=aggregated_competencies,
        competency_weights=competency_weights,
        collaboration_results=collaboration_results,
        low_confidence_list=low_confidence_list
    )

    # í›„ì²˜ë¦¬: ê¸/ë¶€ í‚¤ì›Œë“œ, ì¶”ì²œì§ˆë¬¸, ì „ì²´ ìš”ì•½ ìƒì„± (ê·œì¹™ ê¸°ë°˜)
    analysis_summary = post_processing_service.build_analysis_summary(
        aggregated_competencies=aggregated_competencies,
        final_result=final_result
    )

    duration = (datetime.now() - start_time).total_seconds()

    execution_log = {
        "node": "final_integration",
        "duration_seconds": round(duration, 2),
        "timestamp": datetime.now().isoformat(),
        "status": "success"
    }

    return {
        "final_score": final_result.get("final_score"),
        "avg_confidence": final_result.get("avg_confidence"),
        "final_reliability": final_result.get("reliability", {}).get("level"),
        "reliability_note": final_result.get("reliability", {}).get("note"),
        "final_result": final_result,
        "analysis_summary": analysis_summary,
        "post_processing": {
            "version": "postproc_v1",
            "source": "rules_over_llm_fallback",
            "llm_used": False
        },
        "execution_logs": state.get("execution_logs", []) + [execution_log]
    }
