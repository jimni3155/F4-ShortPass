"""
í‰ê°€ ì„œë¹„ìŠ¤ (LangGraph í†µí•©)
"""

import os
import json
from datetime import datetime
from typing import Dict, Optional
from pathlib import Path
from dotenv import load_dotenv
import asyncio
from ai.agents.graph.evaluation import create_evaluation_graph
from services.storage.s3_service import S3Service
from sqlalchemy.orm import Session
from db.database import SessionLocal
from models.evaluation import Evaluation
from models.interview import Applicant, Company, InterviewSession, InterviewStatus
from models.job import Job

env_path = Path(__file__).parent.parent.parent / '.env'
load_dotenv(env_path)

from ai.prompts.competency_agents.common_competencies.problem_solving_prompt import create_problem_solving_evaluation_prompt
from ai.prompts.competency_agents.common_competencies.organizational_fit_prompt import create_organizational_fit_evaluation_prompt
from ai.prompts.competency_agents.common_competencies.growth_potential_prompt import create_growth_potential_evaluation_prompt
from ai.prompts.competency_agents.common_competencies.interpersonal_skill_prompt import create_interpersonal_skill_evaluation_prompt
from ai.prompts.competency_agents.common_competencies.achievement_motivation_prompt import create_achievement_motivation_evaluation_prompt

from ai.prompts.competency_agents.job_competencies.customer_journey_marketing_prompt import create_customer_journey_marketing_evaluation_prompt
from ai.prompts.competency_agents.job_competencies.data_analysis_prompt import create_md_data_analysis_evaluation_prompt
from ai.prompts.competency_agents.job_competencies.seasonal_strategy_kpi_prompt import create_seasonal_strategy_kpi_evaluation_prompt
from ai.prompts.competency_agents.job_competencies.stakeholder_collaboration_prompt import create_stakeholder_collaboration_evaluation_prompt
from ai.prompts.competency_agents.job_competencies.value_chain_optimization_prompt import create_value_chain_optimization_evaluation_prompt

from openai import AsyncOpenAI

PROMPT_GENERATORS = {
    # Common Competencies (5ê°œ)
    "problem_solving": create_problem_solving_evaluation_prompt,
    "organizational_fit": create_organizational_fit_evaluation_prompt,
    "growth_potential": create_growth_potential_evaluation_prompt,
    "interpersonal_skill": create_interpersonal_skill_evaluation_prompt, 
    "achievement_motivation": create_achievement_motivation_evaluation_prompt,
    
    # Job Competencies (5ê°œ)
    "customer_journey_marketing": create_customer_journey_marketing_evaluation_prompt,
    "md_data_analysis": create_md_data_analysis_evaluation_prompt,
    "seasonal_strategy_kpi": create_seasonal_strategy_kpi_evaluation_prompt,
    "stakeholder_collaboration": create_stakeholder_collaboration_evaluation_prompt,
    "value_chain_optimization": create_value_chain_optimization_evaluation_prompt,
}


class EvaluationService:
    """í‰ê°€ ì„œë¹„ìŠ¤"""
    
    def __init__(self, api_key: str = None):
        if api_key is None:
            api_key = os.getenv("OPENAI_API_KEY")
        
        from core.config import S3_BUCKET_NAME, AWS_REGION
        self.openai_client = AsyncOpenAI(api_key=api_key)
        self.graph = create_evaluation_graph()
        self.s3_service = S3Service(bucket_name=S3_BUCKET_NAME, region_name=AWS_REGION)
    
    def _load_prompts(self, transcript: Dict) -> Dict[str, str]:
        """í”„ë¡¬í”„íŠ¸ ë¡œë”©"""
        transcript_str = json.dumps(transcript, ensure_ascii=False, indent=2)
        return {
            name: generator(transcript_str)
            for name, generator in PROMPT_GENERATORS.items()
        }
    
    def _ensure_required_fields(self, result: Dict) -> Dict:
        """
        í•„ìˆ˜ í•„ë“œ ê°•ì œ ë³´ì¥ (Post-processing)
        
        ëˆ„ë½ëœ í•„ë“œë¥¼ ìë™ ìƒì„±:
            1. key_observations (ì—­ëŸ‰ë³„ í•µì‹¬ ê´€ì°°)
            2. resume_verification_summary (Resume ê²€ì¦ ê·¼ê±°)
            3. overall_evaluation_summary (ì¢…í•© ì‹¬ì‚¬í‰)
        """
        print("\n" + "="*60)
        print("[í•„ìˆ˜ í•„ë“œ ê²€ì¦] ì‹œì‘...")
        print("="*60)
        
        # ========================================
        # 1. Stage 2: aggregated_competencies ê²€ì¦
        # ========================================
        aggregated_competencies = result.get("aggregated_competencies", {})
        
        for comp_name, comp_data in aggregated_competencies.items():
            print(f"\n[{comp_name}] ê²€ì¦ ì¤‘...")
            
            # 1-1. key_observations ë³´ì¥
            if "key_observations" not in comp_data or not comp_data.get("key_observations"):
                print(f"  âš ï¸  key_observations ëˆ„ë½ â†’ ìë™ ìƒì„±")
                
                key_obs = []
                # Strengthsì—ì„œ ì¶”ì¶œ (ìµœëŒ€ 2ê°œ)
                strengths = comp_data.get("strengths", [])
                key_obs.extend(strengths[:2])
                
                # Weaknessesì—ì„œ ì¶”ì¶œ (ìµœëŒ€ 1ê°œ)
                weaknesses = comp_data.get("weaknesses", [])
                if weaknesses:
                    key_obs.append(weaknesses[0])
                
                # ìµœì†Œ 3ê°œ ë³´ì¥
                if len(key_obs) < 3:
                    score = comp_data.get("overall_score", 0)
                    conf_v2 = comp_data.get("confidence_v2", 0.5)
                    
                    if score >= 75:
                        key_obs.append(f"ì‹ ì… ê¸°ì¤€ ìš°ìˆ˜í•œ ìˆ˜ì¤€ (ì ìˆ˜: {score}, ì‹ ë¢°ë„: {conf_v2:.2f})")
                    elif score >= 60:
                        key_obs.append(f"ì‹ ì… ê¸°ì¤€ ì–‘í˜¸í•œ ìˆ˜ì¤€ (ì ìˆ˜: {score}, ì‹ ë¢°ë„: {conf_v2:.2f})")
                    elif score >= 50:
                        key_obs.append(f"ì‹ ì… ê¸°ì¤€ í‰ê·  ìˆ˜ì¤€ (ì ìˆ˜: {score}, ì‹ ë¢°ë„: {conf_v2:.2f})")
                    else:
                        key_obs.append(f"ì‹ ì… ê¸°ì¤€ ë¯¸í¡í•œ ìˆ˜ì¤€ (ì ìˆ˜: {score}, ì‹ ë¢°ë„: {conf_v2:.2f})")
                
                # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œ
                key_obs = list(dict.fromkeys(key_obs))[:5]
                comp_data["key_observations"] = key_obs
                
                print(f"    â†’ ìƒì„±ë¨ ({len(key_obs)}ê°œ)")
            else:
                print(f"  âœ… key_observations ì¡´ì¬ ({len(comp_data['key_observations'])}ê°œ)")
            
            
            # 1-2. resume_verification_summary ë³´ì¥
            if "resume_verification_summary" not in comp_data:
                print(f"  âš ï¸  resume_verification_summary ëˆ„ë½ â†’ ìƒì„±")
                
                # segment_evaluations_with_resumeì—ì„œ ì¶”ì¶œ
                segment_evals = result.get("segment_evaluations_with_resume", [])
                comp_segments = [s for s in segment_evals if s.get("competency") == comp_name]
                
                verified_segments = [
                    s for s in comp_segments 
                    if s.get("resume_verified", False)
                ]
                
                high_strength_segments = [
                    s for s in verified_segments
                    if s.get("verification_strength") == "high"
                ]
                
                # ê²€ì¦ ê°•ë„ ìˆœ ì •ë ¬
                strength_order = {"high": 3, "medium": 2, "low": 1, "none": 0}
                verified_segments_sorted = sorted(
                    verified_segments,
                    key=lambda s: strength_order.get(s.get("verification_strength", "none"), 0),
                    reverse=True
                )
                
                # ìƒìœ„ 3ê°œë§Œ ì¶”ì¶œ
                key_evidence = []
                for seg in verified_segments_sorted[:3]:
                    key_evidence.append({
                        "segment_id": seg.get("segment_id"),
                        "quote": seg.get("quote_text", ""),
                        "resume_section": "unknown",
                        "matched_content": ", ".join(seg.get("resume_evidence", [])),
                        "verification_strength": seg.get("verification_strength", "none"),
                        "reasoning": f"Resume ì¦ê±°: {', '.join(seg.get('resume_evidence', []))}"
                    })
                
                comp_data["resume_verification_summary"] = {
                    "verified_count": len(verified_segments),
                    "high_strength_count": len(high_strength_segments),
                    "key_evidence": key_evidence
                }
                
                print(f"    â†’ ìƒì„±ë¨ (verified: {len(verified_segments)}ê°œ)")
            else:
                verified = comp_data["resume_verification_summary"].get("verified_count", 0)
                print(f"  âœ… resume_verification_summary ì¡´ì¬ ({verified}ê°œ ê²€ì¦)")
        
        result["aggregated_competencies"] = aggregated_competencies
        
        
        # ========================================
        # 2. Stage 3: final_result ê²€ì¦
        # ========================================
        final_result = result.get("final_result", {})
        
        # 2-1. overall_evaluation_summary ë³´ì¥
        if "overall_evaluation_summary" not in final_result or not final_result.get("overall_evaluation_summary"):
            print(f"\nâš ï¸  overall_evaluation_summary ëˆ„ë½ â†’ Fallback ìƒì„±")
            
            final_score = result.get("final_score", 0)
            avg_confidence = result.get("avg_confidence", 0.5)
            
            # ê°„ë‹¨í•œ í…œí”Œë¦¿ ì‹¬ì‚¬í‰
            fallback = (
                f"ì§€ì›ìëŠ” íŒ¨ì…˜ MD ì§ë¬´ì— í•„ìš”í•œ ì—­ëŸ‰ì„ ì „ë°˜ì ìœ¼ë¡œ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤ "
                f"(ì¢…í•© ì ìˆ˜: {final_score:.1f}ì , í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}). "
                f"ì‹ ì… ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ë˜ë©°, ì…ì‚¬ í›„ ì²´ê³„ì ì¸ êµìœ¡ê³¼ ë©˜í† ë§ì„ í†µí•´ ì„±ì¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤."
            )
            final_result["overall_evaluation_summary"] = fallback
            
            print(f"  â†’ ìƒì„±ë¨ ({len(fallback)} chars)")
        else:
            print(f"\nâœ… overall_evaluation_summary ì¡´ì¬ ({len(final_result['overall_evaluation_summary'])} chars)")
        
        
        # 2-2. competency_detailsì— key_observations ë° resume_verification_summary í¬í•¨
        competency_details = final_result.get("competency_details", {})
        
        for comp_name, comp_detail in competency_details.items():
            # aggregated_competenciesì—ì„œ ë³µì‚¬
            if comp_name in aggregated_competencies:
                agg_comp = aggregated_competencies[comp_name]
                
                if "key_observations" not in comp_detail:
                    comp_detail["key_observations"] = agg_comp.get("key_observations", [])
                
                if "resume_verification_summary" not in comp_detail:
                    comp_detail["resume_verification_summary"] = agg_comp.get("resume_verification_summary", {
                        "verified_count": 0,
                        "high_strength_count": 0,
                        "key_evidence": []
                    })
        
        final_result["competency_details"] = competency_details
        result["final_result"] = final_result
        
        
        print("\n" + "="*60)
        print("[í•„ìˆ˜ í•„ë“œ ê²€ì¦] ì™„ë£Œ")
        print("="*60 + "\n")
        
        return result
    
    async def evaluate_interview(
        self,
        interview_id: int,
        applicant_id: int,
        job_id: int,
        transcript: Dict,
        competency_weights: Dict[str, float], 
        resume_data: Optional[Dict] = None 
    ) -> Dict:
        """
        ë©´ì ‘ í‰ê°€ ì‹¤í–‰
        
        Args:
            interview_id: ë©´ì ‘ ID
            applicant_id: ì§€ì›ì ID
            job_id: JD ID
            transcript: ë©´ì ‘ Transcript JSON
            competency_weights: 10ê°œ ì—­ëŸ‰ ê°€ì¤‘ì¹˜
            resume_data: íŒŒì‹±ëœ Resume JSON (ì„ íƒì )
        
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        
        transcript_content = transcript
        transcript_s3_url = f"s3://{self.s3_service.bucket_name}/transcripts/{interview_id}_mock.json"
        prompts = self._load_prompts(transcript_content)

        # Initial State êµ¬ì„±
        initial_state = {
            # ê¸°ë³¸ ì •ë³´
            "interview_id": interview_id,
            "applicant_id": applicant_id,
            "job_id": job_id,
            "transcript_s3_url": transcript_s3_url,
            "transcript_content": transcript_content,
            "transcript": transcript_content,
            "resume_data": resume_data, 
            "openai_client": self.openai_client,
            "prompts": prompts,
            
            # ê°€ì¤‘ì¹˜ 
            "competency_weights": competency_weights,
            
            # Stage 1 ê²°ê³¼ (ì´ˆê¸°í™”)
            "achievement_motivation_result": None,
            "growth_potential_result": None,
            "interpersonal_skill_result": None, 
            "organizational_fit_result": None,
            "problem_solving_result": None,
            "customer_journey_marketing_result": None, 
            "md_data_analysis_result": None, 
            "seasonal_strategy_kpi_result": None, 
            "stakeholder_collaboration_result": None, 
            "value_chain_optimization_result": None, 
            
            # Stage 2 ê²°ê³¼ (ì´ˆê¸°í™”)
            "segment_evaluations_with_resume": [],
            "confidence_v2_calculated": False,
            "segment_overlap_adjustments": [],
            "cross_competency_flags": [],
            "aggregated_competencies": {},
            "low_confidence_list": [],
            "requires_collaboration": False,
            
            # Stage 3 ê²°ê³¼ (ì´ˆê¸°í™”)
            "collaboration_results": [],
            "collaboration_count": 0,
            
            # Final ê²°ê³¼ (ì´ˆê¸°í™”)
            "final_score": None,
            "avg_confidence": None,
            "final_reliability": None,
            "reliability_note": None,
            "final_result": None,
            
            # ë©”íƒ€ ì •ë³´
            "started_at": datetime.now(),
            "completed_at": None,
            "errors": [],
            "execution_logs": []
        }
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        print("\n" + "="*80)
        print(f"í‰ê°€ ì‹œì‘: Interview ID {interview_id}")
        print("="*80)
        
        result = await self.graph.ainvoke(initial_state)
        
        # ğŸ†• í•„ìˆ˜ í•„ë“œ ê°•ì œ ë³´ì¥
        result = self._ensure_required_fields(result)
        
        # ê³µí†µ ëŸ° íƒ€ì„ìŠ¤íƒ¬í”„
        run_timestamp = result.get("started_at")
        if isinstance(run_timestamp, datetime):
            run_ts_str = run_timestamp.strftime("%Y%m%dT%H%M%S")
        else:
            run_ts_str = datetime.now().strftime("%Y%m%dT%H%M%S")
        evaluation_base_prefix = f"evaluations/{interview_id}/{run_ts_str}"

        # S3 ì—…ë¡œë“œ
        execution_logs_s3_key = f"logs/evaluations/{applicant_id}/{interview_id}/{run_ts_str}_execution_logs.json"
        agent_logs_s3_url = self.s3_service.upload_json(
            execution_logs_s3_key, result["execution_logs"]
        )

        stage1_key = f"{evaluation_base_prefix}/stage1_evidence.json"
        stage1_evidence_url = self.s3_service.upload_json(
            stage1_key,
            result.get("aggregated_competencies", {})
        )

        stage2_payload = {
            "segment_evaluations_with_resume": result.get("segment_evaluations_with_resume", []),
            "confidence_v2_calculated": result.get("confidence_v2_calculated", False),
            "segment_overlap_adjustments": result.get("segment_overlap_adjustments", []),
            "cross_competency_flags": result.get("cross_competency_flags", []),
            "aggregated_competencies": result.get("aggregated_competencies", {}),
            "low_confidence_list": result.get("low_confidence_list", []),
            "requires_collaboration": result.get("requires_collaboration", False),
            "competency_weights": competency_weights,
        }
        stage2_key = f"{evaluation_base_prefix}/stage2_aggregator.json"
        stage2_aggregator_url = self.s3_service.upload_json(stage2_key, stage2_payload)

        stage3_payload = {
            "final_score": result.get("final_score"),
            "avg_confidence": result.get("avg_confidence"),
            "final_reliability": result.get("final_reliability"),
            "reliability_note": result.get("reliability_note"),
            "final_result": result.get("final_result"),
            "aggregated_competencies": result.get("aggregated_competencies", {}),
            "collaboration_results": result.get("collaboration_results", []),
            "analysis_summary": result.get("analysis_summary"),
            "post_processing": result.get("post_processing"),
        }
        stage3_key = f"{evaluation_base_prefix}/stage3_final_integration.json"
        stage3_final_url = self.s3_service.upload_json(stage3_key, stage3_payload)

        # DB ì €ì¥
        db = SessionLocal()
        try:
            evaluation_record = await asyncio.to_thread(
                self._save_evaluation_to_db, 
                db, 
                result, 
                transcript_s3_url, 
                agent_logs_s3_url,
                stage1_evidence_url,
                stage2_aggregator_url,
                stage3_final_url,
                run_ts_str
            )
            evaluation_id = evaluation_record.id
        finally:
            await asyncio.to_thread(db.close)
        
        print("\n" + "="*80)
        print("í‰ê°€ ì™„ë£Œ")
        print("="*80)
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        return {
            "evaluation_id": evaluation_id,
            "interview_id": interview_id,
            "applicant_id": applicant_id,
            "job_id": job_id,
            "transcript_s3_url": transcript_s3_url,
            "agent_logs_s3_url": agent_logs_s3_url,
            "stage1_evidence_s3_url": stage1_evidence_url,
            "stage2_aggregator_s3_url": stage2_aggregator_url,
            "stage3_final_integration_s3_url": stage3_final_url,
            "evaluation_run_ts": run_ts_str,
            
            "execution_logs": result.get("execution_logs", []),
            "segment_evaluations_with_resume": result.get("segment_evaluations_with_resume", []),
            "segment_overlap_adjustments": result.get("segment_overlap_adjustments", []),
            "cross_competency_flags": result.get("cross_competency_flags", []),
            "aggregated_competencies": result.get("aggregated_competencies", {}),
            
            "final_score": result.get("final_score"),
            "avg_confidence": result.get("avg_confidence"),
            "final_reliability": result.get("final_reliability"),
            "reliability_note": result.get("reliability_note"),
            "analysis_summary": result.get("analysis_summary"),
            "post_processing": result.get("post_processing"),
            
            "competency_details": result.get("final_result", {}).get("competency_details", {}),
            "low_confidence_summary": result.get("final_result", {}).get("low_confidence_summary", {}),
            "collaboration_summary": result.get("final_result", {}).get("collaboration_summary", {}),
            
            "started_at": result["started_at"].isoformat(),
            "completed_at": datetime.now().isoformat()
        }

    def _save_evaluation_to_db(
        self, 
        db: Session, 
        state: Dict, 
        transcript_s3_url: str, 
        agent_logs_s3_url: str,
        evidence_s3_url: str,
        stage2_aggregator_s3_url: str,
        stage3_final_s3_url: str,
        evaluation_run_ts: str
    ):
        """í‰ê°€ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        
        applicant_id = state["applicant_id"]
        job_id = state["job_id"]

        applicant = db.query(Applicant).filter(Applicant.id == applicant_id).first()
        if not applicant:
            applicant = Applicant(
                id=applicant_id,
                name=f"Applicant {applicant_id}",
                email=f"applicant{applicant_id}@example.com"
            )
            db.add(applicant)

        job = db.query(Job).filter(Job.id == job_id).first()
        if not job:
            company = db.query(Company).filter(Company.id == 1).first()
            if not company:
                company = Company(id=1, name="Default Company")
                db.add(company)
            job = Job(
                id=job_id,
                company_id=company.id,
                title=f"Job {job_id}",
                description="Placeholder job for testing"
            )
            db.add(job)

        interview_id = state.get("interview_id")
        if interview_id:
            interview = db.query(InterviewSession).filter(InterviewSession.id == interview_id).first()
            if not interview:
                interview = InterviewSession(
                    id=interview_id,
                    applicant_id=applicant_id,
                    company_id=job.company_id,
                    status=InterviewStatus.COMPLETED
                )
                db.add(interview)

        db.flush()

        aggregated_competencies = state.get("aggregated_competencies", {})
        final_result = state.get("final_result", {})
        analysis_summary = state.get("analysis_summary")
        post_processing = state.get("post_processing")
        aggregated_eval_payload = {
            "final_result": final_result,
            "analysis_summary": analysis_summary,
            "post_processing": post_processing
        }
        
        evaluation_metadata = {
            "resume_verified_count": sum(
                comp.get("resume_verified_count", 0)
                for comp in aggregated_competencies.values()
            ),
            "segment_overlap_adjustments": len(state.get("segment_overlap_adjustments", [])),
            "collaboration_count": state.get("collaboration_count", 0),
            "s3_paths": {
                "stage1_evidence": evidence_s3_url,
                "stage2_aggregator": stage2_aggregator_s3_url,
                "stage3_final_integration": stage3_final_s3_url,
                "execution_logs": agent_logs_s3_url,
            },
            "evaluation_run_ts": evaluation_run_ts,
            "evaluation_prefix": f"evaluations/{state.get('interview_id')}/{evaluation_run_ts}"
        }
        
        evaluation_record = Evaluation(
            applicant_id=state["applicant_id"],
            job_id=state["job_id"],
            interview_id=state["interview_id"],
            evaluation_status="completed",
            
            match_score=state.get("final_score", 0.0),
            normalized_score=state.get("final_score", 0.0),
            weighted_score=state.get("final_score", 0.0),
            confidence_score=state.get("avg_confidence", 0.0),

            transcript_s3_url=transcript_s3_url,
            agent_logs_s3_url=agent_logs_s3_url,
            evidence_s3_url=evidence_s3_url,

            problem_solving=aggregated_competencies.get("problem_solving", {}).get("overall_score", 0),
            organizational_fit=aggregated_competencies.get("organizational_fit", {}).get("overall_score", 0),
            growth_potential=aggregated_competencies.get("growth_potential", {}).get("overall_score", 0),
            interpersonal_skills=aggregated_competencies.get("interpersonal_skill", {}).get("overall_score", 0),
            achievement_motivation=aggregated_competencies.get("achievement_motivation", {}).get("overall_score", 0),
            
            structured_thinking=aggregated_competencies.get("customer_journey_marketing", {}).get("overall_score", 0),
            business_documentation=aggregated_competencies.get("md_data_analysis", {}).get("overall_score", 0),
            financial_literacy=aggregated_competencies.get("seasonal_strategy_kpi", {}).get("overall_score", 0),
            industry_learning=aggregated_competencies.get("stakeholder_collaboration", {}).get("overall_score", 0),
            stakeholder_management=aggregated_competencies.get("value_chain_optimization", {}).get("overall_score", 0),

            job_aggregation=None,
            common_aggregation=None,
            
            validation_result={
                "low_confidence_list": state.get("low_confidence_list", []),
                "cross_competency_flags": state.get("cross_competency_flags", []),
            },

            competency_scores={
                comp_name: comp_data.get("overall_score", 0)
                for comp_name, comp_data in aggregated_competencies.items()
            },
            individual_evaluations=aggregated_competencies,
            aggregated_evaluation=aggregated_eval_payload,
            match_result={
                "final_score": state.get("final_score"),
                "reliability": state.get("final_reliability"),
                "reliability_note": state.get("reliability_note")
            },
            reasoning_log={},
            rubric_scores={},
            normalization_metadata={},
            evaluation_metadata=evaluation_metadata,
            
            created_at=state["started_at"],
            updated_at=datetime.now()
        )
        db.add(evaluation_record)
        db.commit()
        db.refresh(evaluation_record)
        return evaluation_record
