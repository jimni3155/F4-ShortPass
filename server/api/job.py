# server/api/job.py
"""
Job ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.orm import Session
from typing import Optional, List
import logging

from db.database import get_db
from services.job_service import JobService
from schemas.evaluation import ApplicantListResponse
from pydantic import BaseModel


# Mock data, converted from the frontend mock file
candidate_list_mock = {
  "company_name": "ì‚¼ì„±ë¬¼ì‚° íŒ¨ì…˜ë¶€ë¬¸",
  "job_title": "ìƒí’ˆê¸°íš(MD) / Retailì˜ì—…",
  "total_applicants": 5,
  "completed_evaluations": 3,
  "average_score": 83.3,
  "applicants": [
    {
      "applicant_id": "CAND_001",
      "job_id": "JOB_001",
      "rank": 1,
      "applicant_name": "ê¹€ì§€ì›",
      "track": "ìƒí’ˆê¸°íš(MD)",
      "total_score": 92,
      "strengths": "Data-Driven Insight",
      "weaknesses": "Global Mindset",
      "ai_summary_comment": "ë°ì´í„° ê¸°ë°˜ MDë¡œ ì¬ê³ íšŒì „ìœ¨ 0.8â†’1.2 ê°œì„ (í’ˆì ˆë¥  5%â†“), ë§ˆì§„ 35â†’38.5% ë‹¬ì„±í•˜ë©° ì›ê°€Â·í’ˆì§ˆ ë¦¬ìŠ¤í¬ ê´€ë¦¬, ë””ìì¸/VMDÂ·ê³µê¸‰ì—…ì²´ í˜‘ì—…Â·í˜‘ìƒì— ê°•ì ì´ê³  ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì²´ê³„ í•™ìŠµ ì˜ì§€ê°€ ëª…í™•í•œ ì§€ì›ì.",
      "status": "ğŸŸ¢ ì¶”ì²œ",
      "competency_scores": [
          {"name": "Data Insight", "score": 95},
          {"name": "Strategic Solving", "score": 90},
          {"name": "Value Chain", "score": 85},
          {"name": "Marketing", "score": 88},
          {"name": "Stakeholder", "score": 80}
      ]
    },
    {
      "applicant_id": "CAND_002",
      "job_id": "JOB_001",
      "rank": 2,
      "applicant_name": "ì´ì‚¼ì„±",
      "track": "Retailì˜ì—…",
      "total_score": 88,
      "strengths": "Stakeholder Mgmt",
      "weaknesses": "Creativity & Execution",
      "ai_summary_comment": "ìœ ê´€ë¶€ì„œ ì„¤ë“ ë…¼ë¦¬ê°€ ëª…í™•í•˜ë‚˜, ìœ„ê¸° ìƒí™© ëŒ€ì²˜ì˜ êµ¬ì²´ì„±ì´ ë‹¤ì†Œ ë¶€ì¡±í•¨.",
      "status": "ğŸŸ¢ ì¶”ì²œ",
      "competency_scores": [
          {"name": "Data Insight", "score": 80},
          {"name": "Strategic Solving", "score": 85},
          {"name": "Value Chain", "score": 88},
          {"name": "Marketing", "score": 90},
          {"name": "Stakeholder", "score": 95}
      ]
    },
    {
      "applicant_id": "CAND_003",
      "job_id": "JOB_001",
      "rank": 3,
      "applicant_name": "ë°•ë¬¼ì‚°",
      "track": "ìƒí’ˆê¸°íš(MD)",
      "total_score": 74,
      "strengths": "Value Chain Optimization",
      "weaknesses": "Strategic Problem Solving",
      "ai_summary_comment": "ì‹¤ë¬´ ê²½í—˜ì€ í’ë¶€í•˜ë‚˜, ì „ëµì  ì˜ì‚¬ê²°ì •ì˜ ê·¼ê±°ê°€ ì§ê´€ì— ì˜ì¡´í•¨.",
      "status": "ğŸŸ¡ ë³´ë¥˜",
      "competency_scores": [
          {"name": "Data Insight", "score": 60},
          {"name": "Strategic Solving", "score": 65},
          {"name": "Value Chain", "score": 90},
          {"name": "Marketing", "score": 75},
          {"name": "Stakeholder", "score": 80}
      ]
    },
    {
      "applicant_id": "CAND_004",
      "job_id": "JOB_001",
      "rank": 4,
      "applicant_name": "ìµœí˜ì‹ ",
      "track": "Retailì˜ì—…",
      "total_score": 65,
      "strengths": "Customer Journey & Marketing Strategy",
      "weaknesses": "Data-Driven Insight",
      "ai_summary_comment": "ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½ì— ê°•ì ì´ë‚˜, ë°ì´í„° ë¶„ì„ ëŠ¥ë ¥ í–¥ìƒ í•„ìš”.",
      "status": "ğŸŸ  ê²€í†  í•„ìš”",
      "competency_scores": [
          {"name": "Data Insight", "score": 50},
          {"name": "Strategic Solving", "score": 60},
          {"name": "Value Chain", "score": 70},
          {"name": "Marketing", "score": 85},
          {"name": "Stakeholder", "score": 60}
      ]
    },
    {
      "applicant_id": "CAND_005",
      "job_id": "JOB_001",
      "rank": 5,
      "applicant_name": "ì •ì—´ì •",
      "track": "ìƒí’ˆê¸°íš(MD)",
      "total_score": 58,
      "strengths": "Creativity & Execution",
      "weaknesses": "All competencies need improvement",
      "ai_summary_comment": "ì—´ì •ì ì´ë‚˜, ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰ ì „ë°˜ì— ê±¸ì³ ë³´ì™„ì´ í•„ìš”í•¨.",
      "status": "ğŸ”´ ë¯¸í¡",
      "competency_scores": [
          {"name": "Data Insight", "score": 55},
          {"name": "Strategic Solving", "score": 50},
          {"name": "Value Chain", "score": 60},
          {"name": "Marketing", "score": 65},
          {"name": "Stakeholder", "score": 60}
      ]
    }
  ]
}


router = APIRouter(prefix="/jobs", tags=["jobs"])
logger = logging.getLogger("uvicorn")

@router.get("/{job_id}/applicants", response_model=ApplicantListResponse)
async def get_applicants_for_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • Jobì— ëŒ€í•œ ì§€ì›ì ëª©ë¡ ë° í‰ê°€ ìš”ì•½ ì¡°íšŒ

    Args:
        job_id: Job ID

    Returns:
        ApplicantListResponse: ì§€ì›ì ëª©ë¡ ë° ìš”ì•½ ì •ë³´
    """
    logger.info(f"Getting applicants for job ID: {job_id}")
    
    # TODO: Replace this with actual service call to fetch and build the response
    # For now, returning mock data.
    # Note: In a real implementation, you would check if job_id exists.
    
    # The job_id in the mock is 'JOB_001', but we ignore the input job_id for now
    
    return candidate_list_mock





router = APIRouter(prefix="/jobs", tags=["jobs"])
logger = logging.getLogger("uvicorn")


# Response Models
class JobResponse(BaseModel):
    job_id: int
    company_id: int
    title: str
    created_at: str
    total_chunks: int

    class Config:
        from_attributes = True


class ChunkResponse(BaseModel):
    chunk_id: int
    chunk_text: str
    chunk_index: int
    has_embedding: bool


class JobDetailResponse(BaseModel):
    job_id: int
    company_id: int
    title: str
    description: str
    created_at: str
    chunks: List[ChunkResponse]
    total_chunks: int


class SearchResult(BaseModel):
    chunk_id: int
    job_id: int
    chunk_text: str
    chunk_index: int
    similarity: float


# Endpoints
@router.post("/upload", response_model=JobResponse)
async def upload_jd_pdf(
    pdf_file: UploadFile = File(..., description="JD PDF íŒŒì¼"),
    company_id: int = Form(..., description="íšŒì‚¬ ID"),
    title: str = Form(..., description="ì±„ìš© ê³µê³  ì œëª©"),
    db: Session = Depends(get_db)
):
    """
    JD PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬

    ì „ì²´ í”Œë¡œìš°:
    1. PDF íŒŒì¼ ì—…ë¡œë“œ ë°›ê¸°
    2. S3ì— ì €ì¥
    3. PDF íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    4. ì²­í¬ ë¶„í• 
    5. Bedrock Titanìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    6. DBì— ì €ì¥ (jobs, job_chunks)

    Args:
        pdf_file: PDF íŒŒì¼
        company_id: íšŒì‚¬ ID
        title: ì±„ìš© ê³µê³  ì œëª©

    Returns:
        JobResponse: ìƒì„±ëœ Job ì •ë³´
    """
    logger.info(f"Uploading JD for company ID: {company_id}, title: {title}")
    # PDF íŒŒì¼ ê²€ì¦
    if not pdf_file.filename.endswith('.pdf'):
        raise HTTPException(
            status_code=400,
            detail="Only PDF files are allowed"
        )

    # íŒŒì¼ í¬ê¸° ì œí•œ (ì˜ˆ: 10MB)
    max_size = 10 * 1024 * 1024  # 10MB
    pdf_content = await pdf_file.read()

    if len(pdf_content) > max_size:
        raise HTTPException(
            status_code=400,
            detail=f"File size exceeds maximum limit of {max_size / (1024*1024)}MB"
        )

    # Job ì„œë¹„ìŠ¤ë¡œ ì²˜ë¦¬
    try:
        job_service = JobService()
        job = await job_service.process_jd_pdf(
            db=db,
            pdf_content=pdf_content,
            file_name=pdf_file.filename,
            company_id=company_id,
            title=title
        )

        # ì²­í¬ ê°œìˆ˜ ì¡°íšŒ
        chunk_count = len(job.chunks)

        return JobResponse(
            job_id=job.id,
            company_id=job.company_id,
            title=job.title,
            created_at=job.created_at.isoformat(),
            total_chunks=chunk_count
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process JD PDF: {str(e)}"
        )


@router.get("/{job_id}", response_model=JobDetailResponse)
async def get_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Job ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì²­í¬ í¬í•¨)

    Args:
        job_id: Job ID

    Returns:
        JobDetailResponse: Job ìƒì„¸ ì •ë³´
    """
    logger.info(f"Getting job with ID: {job_id}")
    job_service = JobService()
    job_data = job_service.get_job_with_chunks(db, job_id)

    if not job_data:
        raise HTTPException(status_code=404, detail="Job not found")

    return JobDetailResponse(
        job_id=job_data["job_id"],
        company_id=job_data["company_id"],
        title=job_data["title"],
        description=job_data["description"],
        created_at=job_data["created_at"].isoformat(),
        chunks=[
            ChunkResponse(
                chunk_id=chunk["chunk_id"],
                chunk_text=chunk["chunk_text"],
                chunk_index=chunk["chunk_index"],
                has_embedding=chunk["has_embedding"]
            )
            for chunk in job_data["chunks"]
        ],
        total_chunks=job_data["total_chunks"]
    )


@router.post("/search", response_model=List[SearchResult])
async def search_similar_chunks(
    query: str = Form(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Form(5, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜"),
    job_id: Optional[int] = Form(None, description="íŠ¹ì • Jobìœ¼ë¡œ ì œí•œ"),
    db: Session = Depends(get_db)
):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í¬ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
        job_id: íŠ¹ì • Jobìœ¼ë¡œ ì œí•œ (ì„ íƒ)

    Returns:
        List[SearchResult]: ìœ ì‚¬í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"Searching for similar chunks with query: {query}")
    try:
        job_service = JobService()
        results = job_service.search_similar_chunks(
            db=db,
            query_text=query,
            top_k=top_k,
            job_id=job_id
        )

        return [
            SearchResult(
                chunk_id=r["chunk_id"],
                job_id=r["job_id"],
                chunk_text=r["chunk_text"],
                chunk_index=r["chunk_index"],
                similarity=r["similarity"]
            )
            for r in results
        ]

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )


@router.delete("/{job_id}")
async def delete_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Job ì‚­ì œ (ì²­í¬ë„ í•¨ê»˜ ì‚­ì œë¨)

    Args:
        job_id: Job ID

    Returns:
        Dict: ì‚­ì œ ê²°ê³¼
    """
    logger.info(f"Deleting job with ID: {job_id}")
    job_service = JobService()
    success = job_service.delete_job(db, job_id)

    if not success:
        raise HTTPException(status_code=404, detail="Job not found")

    return {"message": f"Job {job_id} deleted successfully"}


class CompetencyAnalysisResponse(BaseModel):
    """í•µì‹¬ ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼"""
    competencies: List[dict]
    category_weights: dict
    reasoning: str

    class Config:
        from_attributes = True


@router.post("/analyze-competencies", response_model=CompetencyAnalysisResponse)
async def analyze_jd_competencies(
    pdf_file: UploadFile = File(..., description="JD PDF íŒŒì¼"),
    company_id: int = Form(..., description="íšŒì‚¬ ID"),
    company_url: Optional[str] = Form(None, description="íšŒì‚¬ URL (í•µì‹¬ ê°€ì¹˜ ë¶„ì„ìš©)"),
    db: Session = Depends(get_db)
):
    """
    JD PDFì—ì„œ í•µì‹¬ ì—­ëŸ‰ ë¶„ì„

    ì „ì²´ í”Œë¡œìš°:
    1. PDF íŒŒì¼ ì½ê¸° ë° íŒŒì‹±
    2. JD í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì—­ëŸ‰ ì¶”ì¶œ
    3. (ì„ íƒ) company_urlì´ ìˆìœ¼ë©´ RAGë¡œ íšŒì‚¬ ê°€ì¹˜ ê²€ìƒ‰
    4. ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    Args:
        pdf_file: JD PDF íŒŒì¼
        company_id: íšŒì‚¬ ID
        company_url: íšŒì‚¬ ì†Œê°œ URL (ì„ íƒ)

    Returns:
        CompetencyAnalysisResponse: ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"Analyzing JD competencies for company ID: {company_id}")
    # PDF íŒŒì¼ ê²€ì¦
    if not pdf_file.filename.endswith('.pdf'):
        raise HTTPException(
            status_code=400,
            detail="Only PDF files are allowed"
        )

    # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
    max_size = 10 * 1024 * 1024
    pdf_content = await pdf_file.read()

    if len(pdf_content) > max_size:
        raise HTTPException(
            status_code=400,
            detail=f"File size exceeds maximum limit of {max_size / (1024*1024)}MB"
        )

    try:
        print(f"\n{'='*60}")
        print(f"[analyze_jd_competencies] Starting JD competency analysis")
        print(f"{'='*60}")
        print(f"  - Company ID: {company_id}")
        print(f"  - Company URL: {company_url or 'Not provided'}")
        print(f"  - PDF file: {pdf_file.filename}")

        job_service = JobService()

        # 1. PDF íŒŒì‹±
        print("\n[Step 1/3] Parsing PDF...")
        from ai.parsers.jd_parser import JDParser
        jd_parser = JDParser()

        try:
            parsed_result = jd_parser.parse_and_chunk(pdf_content=pdf_content)
            full_text = parsed_result["full_text"]
            print(f"  âœ“ PDF parsed: {len(full_text)} characters")
        except Exception as e:
            print(f"  âœ— PDF parsing failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to parse PDF: {str(e)}"
            )

        # 2. í•µì‹¬ ì—­ëŸ‰ ì¶”ì¶œ
        print(f"\n[Step 2/3] Extracting competencies from JD...")

        try:
            analysis_result = await job_service._extract_company_weights(full_text)
        except Exception as e:
            print(f"  âœ— Competency extraction failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to extract competencies: {str(e)}"
            )

        if not analysis_result:
            print(f"  âœ— No analysis result returned")
            raise HTTPException(
                status_code=500,
                detail="Failed to analyze competencies - no result returned"
            )

        print(f"  âœ“ Competencies extracted: {len(analysis_result.get('competencies', []))} competencies")

        # 3. (ì„ íƒ) Company URLì´ ìˆìœ¼ë©´ DBì— ì—…ë°ì´íŠ¸
        print(f"\n[Step 3/3] Updating company information...")
        if company_url:
            try:
                from models.interview import Company
                company = db.query(Company).filter(Company.id == company_id).first()
                if company:
                    company.company_url = company_url
                    db.commit()
                    print(f"  âœ“ Company URL updated")
                else:
                    print(f"  âš  Company {company_id} not found")
            except Exception as e:
                print(f"  âš  Failed to update company URL: {e}")
                # Continue without failing the entire request
        else:
            print(f"  - No company URL to update")

        print(f"\n{'='*60}")
        print(f"âœ“ Analysis completed successfully!")
        print(f"  - Competencies found: {len(analysis_result.get('competencies', []))}")
        print(f"{'='*60}\n")

        return CompetencyAnalysisResponse(
            competencies=analysis_result.get("competencies", []),
            category_weights=analysis_result.get("weights", {}),
            reasoning=analysis_result.get("reasoning", "")
        )

    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"âœ— Competency analysis failed: {e}")
        print(f"   Error type: {type(e).__name__}")
        print(f"{'='*60}\n")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to analyze competencies: {str(e)}"
        )
