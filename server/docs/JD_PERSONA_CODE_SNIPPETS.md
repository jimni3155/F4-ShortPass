# JD and Persona System - Code Snippets and Architecture

## Part 1: JD Upload Flow - Code Walkthrough

### 1.1 Frontend - Initiate JD Upload
```javascript
// client/src/pages/CompanyInfo.jsx
const handleSave = async () => {
    setLoading(true);
    try {
        // Save company info
        // TODO: implement saveCompany function
        const tempCompanyId = 1;
        setCompanyId(tempCompanyId);
        
        // Navigate to company result page
        navigate(`/company/result/${tempCompanyId}`);
    } catch (err) {
        alert('ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    } finally {
        setLoading(false);
    }
};
```

### 1.2 Backend - API Endpoint
```python
# server/api/job.py
@router.post("/upload", response_model=JobResponse)
async def upload_jd_pdf(
    pdf_file: UploadFile = File(..., description="JD PDF íŒŒì¼"),
    company_id: int = Form(..., description="íšŒì‚¬ ID"),
    title: str = Form(..., description="ì±„ìš© ê³µê³  ì œëª©"),
    db: Session = Depends(get_db)
):
    # Validate PDF
    if not pdf_file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed")
    
    # Check file size (10MB max)
    pdf_content = await pdf_file.read()
    max_size = 10 * 1024 * 1024
    if len(pdf_content) > max_size:
        raise HTTPException(status_code=400, detail=f"File size exceeds {max_size / (1024*1024)}MB")
    
    # Process with JobService
    try:
        job_service = JobService()
        job = await job_service.process_jd_pdf(
            db=db,
            pdf_content=pdf_content,
            file_name=pdf_file.filename,
            company_id=company_id,
            title=title
        )
        
        return JobResponse(
            job_id=job.id,
            company_id=job.company_id,
            title=job.title,
            created_at=job.created_at.isoformat(),
            total_chunks=len(job.chunks)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to process JD PDF: {str(e)}")
```

### 1.3 Service - Main Processing Pipeline
```python
# server/services/job_service.py
async def process_jd_pdf(
    self,
    db: Session,
    pdf_content: bytes,
    file_name: str,
    company_id: int,
    title: str
) -> Job:
    try:
        print(f"\n{'='*60}")
        print(f"Starting JD PDF processing: {file_name}")
        print(f"{'='*60}")

        # Step 1: Upload to S3
        print("\n[Step 1/5] Uploading PDF to S3...")
        s3_key = self.s3_service.upload_file(
            file_content=pdf_content,
            file_name=file_name,
            folder="jd_pdfs"
        )

        # Step 2: Parse PDF and create chunks
        print("\n[Step 2/5] Parsing PDF and creating chunks...")
        parsed_result = self.jd_parser.parse_and_chunk(
            pdf_content=pdf_content,
            metadata={
                "company_id": company_id,
                "s3_key": s3_key,
                "file_name": file_name
            }
        )

        full_text = parsed_result["full_text"]
        chunks = parsed_result["chunks"]

        # Step 2-1: Extract company weights (NEW)
        print("\n[Step 2-1/6] Extracting company weights from JD...")
        weights_data = await self._extract_company_weights(full_text)

        if weights_data and "weights" in weights_data and Company:
            try:
                company = db.query(Company).filter(Company.id == company_id).first()
                if company:
                    company.category_weights = weights_data["weights"]
                    if not company.company_culture_desc and "reasoning" in weights_data:
                        company.company_culture_desc = str(weights_data.get("reasoning", {}))
                    db.flush()
                    print(f"  âœ“ Company weights updated: {weights_data['weights']}")
            except Exception as e:
                print(f"  âš  Failed to update company weights: {e}")

        # Step 3: Create Job record
        print("\n[Step 3/5] Creating Job record...")
        job = Job(
            company_id=company_id,
            title=title,
            description=full_text
        )
        db.add(job)
        db.flush()  # Generate ID
        print(f"  - Job created with ID: {job.id}")

        # Step 4: Generate embeddings
        print("\n[Step 4/5] Generating embeddings for chunks...")
        chunk_texts = [chunk["chunk_text"] for chunk in chunks]
        embeddings = self.embedding_service.generate_embeddings_batch(
            texts=chunk_texts,
            batch_size=5
        )

        # Step 5: Save chunks
        print("\n[Step 5/5] Saving chunks to database...")
        created_chunks = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            if embedding is None:
                print(f"  âš  Skipping chunk {i} (embedding failed)")
                continue

            job_chunk = JobChunk(
                job_id=job.id,
                chunk_text=chunk["chunk_text"],
                embedding=embedding,
                chunk_index=chunk["chunk_index"]
            )
            db.add(job_chunk)
            created_chunks.append(job_chunk)

        db.commit()
        db.refresh(job)

        print(f"\n{'='*60}")
        print(f"âœ“ JD Processing completed successfully!")
        print(f"  - Job ID: {job.id}")
        print(f"  - Chunks saved: {len(created_chunks)}")
        print(f"  - S3 Key: {s3_key}")
        print(f"{'='*60}\n")

        return job

    except Exception as e:
        db.rollback()
        print(f"\nâœ— JD Processing failed: {e}")
        raise Exception(f"Failed to process JD PDF: {str(e)}")
```

---

## Part 2: RAG Agent - JD Parsing

### 2.1 RAG Agent Parse Flow
```python
# server/ai/agents/rag_agent.py
async def parse_jd(
    self,
    job_description: str,
    job_title: str
) -> Dict[str, Any]:
    """
    Extract structured information from JD
    """
    prompt = self._build_jd_parsing_prompt(job_description, job_title)

    try:
        response = await self.llm_client.ainvoke(prompt)

        # Parse JSON response
        response_text = response.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()

        parsed_data = json.loads(response_text)

        # Validate and return
        return self._validate_parsed_data(parsed_data)

    except json.JSONDecodeError as e:
        print(f"RAG Agent: JSON parsing failed - {e}")
        return self._get_default_parsed_data()
    except Exception as e:
        print(f"RAG Agent: JD parsing failed - {e}")
        return self._get_default_parsed_data()
```

### 2.2 RAG Agent Prompt Structure
```python
# server/ai/agents/rag_agent.py
def _build_jd_parsing_prompt(self, job_description: str, job_title: str) -> str:
    return f"""ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì±„ìš©ê³µê³ ì—ì„œ ë©´ì ‘ í‰ê°€ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

<ì±„ìš©ê³µê³ >
ì§ë¬´ëª…: {job_title}

{job_description}
</ì±„ìš©ê³µê³ >

<ì¶”ì¶œ_í•­ëª©>
1. **í•„ìˆ˜ ê¸°ìˆ  (required_skills)**
   - ê³µê³ ì—ì„œ "í•„ìˆ˜", "ë°˜ë“œì‹œ", "required" ë“±ìœ¼ë¡œ ëª…ì‹œëœ ê¸°ìˆ 
   - ìµœëŒ€ 10ê°œ

2. **ìš°ëŒ€ ê¸°ìˆ  (preferred_skills)**
   - ê³µê³ ì—ì„œ "ìš°ëŒ€", "ì„ í˜¸", "preferred" ë“±ìœ¼ë¡œ ëª…ì‹œëœ ê¸°ìˆ 
   - ìµœëŒ€ 5ê°œ

3. **ë„ë©”ì¸ ìš”êµ¬ì‚¬í•­ (domain_requirements)**
   - íŠ¹ì • ì‚°ì—…/ë„ë©”ì¸ ì§€ì‹ ìš”êµ¬ì‚¬í•­

4. **ë™ì  í‰ê°€ ê¸°ì¤€ (dynamic_evaluation_criteria)**
   - ì´ ì§ë¬´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 5ê°œ í‰ê°€ í•­ëª©
   - ë°˜ë“œì‹œ 5ê°œ (ë” ë§ê±°ë‚˜ ì ìœ¼ë©´ ì•ˆ ë¨)

5. **ì—­ëŸ‰ë³„ ê°€ì¤‘ì¹˜ (competency_weights)**
   - 6ê°œ ì—­ëŸ‰ì˜ ì¤‘ìš”ë„ë¥¼ 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ (í•©ê³„ 1.0)
   - job_expertise: ì§ë¬´ ì „ë¬¸ì„±
   - analytical: ë¶„ì„ì  ì‚¬ê³ 
   - execution: ì‹¤í–‰ë ¥
   - relationship: ê´€ê³„ í˜•ì„±
   - resilience: íšŒë³µíƒ„ë ¥ì„±
   - influence: ì˜í–¥ë ¥

6. **í¬ì§€ì…˜ íƒ€ì… (position_type)**
   - ì§ë¬´ ë¶„ë¥˜: backend, frontend, fullstack, devops, data, pm, designer ë“±

7. **ì‹œë‹ˆì–´ë¦¬í‹° ë ˆë²¨ (seniority_level)**
   - junior, mid, senior, lead, principal ì¤‘ í•˜ë‚˜

8. **ì£¼ìš” ì—…ë¬´ (main_responsibilities)**
   - í•µì‹¬ ì—…ë¬´ 3-5ê°œ
</ì¶”ì¶œ_í•­ëª©>

<ì¶œë ¥_í˜•ì‹>
ì˜¤ì§ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

{{
  "required_skills": ["Python", "FastAPI", "PostgreSQL", ...],
  "preferred_skills": ["Kubernetes", "GraphQL", ...],
  "domain_requirements": ["ì´ì»¤ë¨¸ìŠ¤", ...],
  "dynamic_evaluation_criteria": [
    "Python ìˆ™ë ¨ë„",
    "AWS ì¸í”„ë¼ ìš´ì˜",
    "ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
    "ì´ì»¤ë¨¸ìŠ¤ ë„ë©”ì¸ ì§€ì‹",
    "ì‹¤ì‹œê°„ ë¬¸ì œí•´ê²° ëŠ¥ë ¥"
  ],
  "competency_weights": {{
    "job_expertise": 0.40,
    "analytical": 0.15,
    "execution": 0.20,
    "relationship": 0.10,
    "resilience": 0.05,
    "influence": 0.10
  }},
  "position_type": "backend",
  "seniority_level": "senior",
  "main_responsibilities": [...]
}}
</ì¶œë ¥_í˜•ì‹>

<ì¤‘ìš”_ê·œì¹™>
1. dynamic_evaluation_criteriaëŠ” ë°˜ë“œì‹œ ì •í™•íˆ 5ê°œ
2. competency_weightsì˜ í•©ì€ ë°˜ë“œì‹œ 1.0
3. required_skillsì™€ preferred_skillsëŠ” ì¤‘ë³µ ì—†ì´
4. ê³µê³ ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸/ê¸°ë³¸ê°’
</ì¤‘ìš”_ê·œì¹™>

ìœ„ í˜•ì‹ì— ë§ì¶° JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""
```

### 2.3 Validation Logic
```python
# server/ai/agents/rag_agent.py
def _validate_parsed_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
    """Validate and normalize parsed data"""

    validated = {
        "required_skills": data.get("required_skills", []),
        "preferred_skills": data.get("preferred_skills", []),
        "domain_requirements": data.get("domain_requirements", []),
        "dynamic_evaluation_criteria": data.get("dynamic_evaluation_criteria", []),
        "competency_weights": data.get("competency_weights", {}),
        "position_type": data.get("position_type", "unknown"),
        "seniority_level": data.get("seniority_level", "mid"),
        "main_responsibilities": data.get("main_responsibilities", [])
    }

    # Validate dynamic_evaluation_criteria (exactly 5)
    criteria = validated["dynamic_evaluation_criteria"]
    if len(criteria) < 5:
        # Add defaults if missing
        default_criteria = [
            "ê¸°ìˆ  ì „ë¬¸ì„±",
            "ë¬¸ì œ í•´ê²° ëŠ¥ë ¥",
            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬",
            "íŒ€ì›Œí¬",
            "ì„±ì¥ ê°€ëŠ¥ì„±"
        ]
        while len(criteria) < 5:
            criteria.append(default_criteria[len(criteria)])
    else:
        criteria = criteria[:5]  # Use only top 5

    # Validate competency_weights (sum to 1.0)
    weights = validated["competency_weights"]
    required_keys = ["job_expertise", "analytical", "execution", 
                     "relationship", "resilience", "influence"]

    if not all(key in weights for key in required_keys):
        # Use defaults
        validated["competency_weights"] = self._get_default_weights()
    else:
        # Normalize if sum != 1.0
        total = sum(weights.values())
        if abs(total - 1.0) > 0.01:
            validated["competency_weights"] = {
                key: val / total for key, val in weights.items()
            }

    return validated
```

---

## Part 3: Persona Generation Flow

### 3.1 Competency Service - Persona Generation
```python
# server/services/competency_service.py
async def generate_persona_data(
    self,
    jd_text: str,
    job_competencies: List[str],
    company_questions: List[str]
) -> Dict[str, Any]:
    """
    Generate persona data from JD and competencies
    """
    try:
        prompt = self._build_persona_generation_prompt(
            jd_text, job_competencies, company_questions
        )

        response = await self.llm_client.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=2000
        )

        result = self._parse_persona_response(response)

        # Add core competencies and questions
        result["common_competencies"] = self.COMMON_COMPETENCIES
        result["job_competencies"] = job_competencies
        result["core_questions"] = company_questions

        return result

    except Exception as e:
        print(f"Error generating persona: {e}")
        return self._get_default_persona_data(job_competencies, company_questions)
```

### 3.2 Persona Prompt
```python
# server/services/competency_service.py
def _build_persona_generation_prompt(
    self,
    jd_text: str,
    job_competencies: List[str],
    company_questions: List[str]
) -> str:
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(company_questions)])
    competencies_text = ", ".join(job_competencies)

    return f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ 2ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

<ì±„ìš©ê³µê³ >
{jd_text}
</ì±„ìš©ê³µê³ >

<ì§ë¬´ ì—­ëŸ‰>
{competencies_text}
</ì§ë¬´ ì—­ëŸ‰>

<ê¸°ì—… í•„ìˆ˜ ì§ˆë¬¸>
{questions_text}
</ê¸°ì—… í•„ìˆ˜ ì§ˆë¬¸>

ìš”êµ¬ì‚¬í•­:
1. ì„œë¡œ ë‹¤ë¥¸ í‰ê°€ ì´ˆì ì„ ê°€ì§„ 2ê°œì˜ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ìƒì„±
2. ê° í˜ë¥´ì†Œë‚˜ëŠ” ì§ë¬´ ì—­ëŸ‰ ì¤‘ 2-3ê°œë¥¼ ì¤‘ì ì ìœ¼ë¡œ í‰ê°€
3. ì‹¤ì œ ë©´ì ‘ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì§ˆë¬¸ í¬í•¨

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "company": "íšŒì‚¬ëª… ì¶”ì¶œ",
  "persona_summary": [
    {{
      "type": "ë…¼ë¦¬í˜• ë©´ì ‘ê´€",
      "focus": "ë¬¸ì œí•´ê²°ë ¥ê³¼ ë¶„ì„ì  ì‚¬ê³ ë¥¼ ì¤‘ì  í‰ê°€",
      "target_competencies": ["ë¬¸ì œí•´ê²°ë ¥", "ë¶„ì„ì  ì‚¬ê³ "],
      "example_question": "í”„ë¡œì íŠ¸ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜ìš”?"
    }},
    {{
      "type": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜í˜• ë©´ì ‘ê´€",
      "focus": "í˜‘ì—… ë° ì†Œí†µ ëŠ¥ë ¥ í‰ê°€",
      "target_competencies": ["ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ë¦¬ë”ì‹­"],
      "example_question": "ì˜ê²¬ ì¶©ëŒì´ ìˆì—ˆì„ ë•Œ, ì–´ë–»ê²Œ ì¡°ìœ¨í–ˆë‚˜ìš”?"
    }}
  ]
}}

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
```

### 3.3 JD Persona Service - Complete Flow
```python
# server/services/jd_persona_service.py
async def create_and_save_persona(
    self,
    db: Session,
    job_id: int,
    company_id: int,
    jd_text: str,
    company_questions: List[str]
) -> Dict[str, Any]:
    """
    Complete persona creation and saving flow
    """
    try:
        print(f"ğŸ­ Starting persona creation for Job {job_id}")

        # Step 1: Analyze competencies
        competency_data = await self.competency_service.analyze_jd_competencies(jd_text)
        print(f" Extracted competencies: {len(competency_data['job_competencies'])} job-specific")

        # Step 2: Generate persona data
        persona_data = await self.competency_service.generate_persona_data(
            jd_text=jd_text,
            job_competencies=competency_data["job_competencies"],
            company_questions=company_questions
        )

        # Step 3: Create visualization data
        visualization_data = self.competency_service.get_competency_visualization_data(
            job_competencies=competency_data["job_competencies"]
        )

        # Step 4: Merge all data
        complete_persona_data = {
            **persona_data,
            "analysis_summary": competency_data.get("analysis_summary", "")
        }

        # Step 5: Save to DB
        jd_persona = JDPersona.create_from_generation_result(
            job_id=job_id,
            company_id=company_id,
            generation_result=complete_persona_data,
            visualization_data=visualization_data
        )

        db.add(jd_persona)
        db.commit()
        db.refresh(jd_persona)

        print(f"âœ… Persona saved to DB with ID: {jd_persona.id}")

        # Step 6: Return complete result
        result = jd_persona.to_dict()
        result["visualization_data"] = visualization_data

        return result

    except Exception as e:
        db.rollback()
        print(f"âŒ Failed to create persona: {e}")
        raise Exception(f"Failed to create persona: {str(e)}")
```

---

## Part 4: Database Schema Visualization

### 4.1 Database Tables Relationship
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          jobs (ì±„ìš©ê³µê³ )             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                             â”‚
â”‚ company_id                          â”‚
â”‚ title                               â”‚
â”‚ description (full JD text)          â”‚
â”‚                                     â”‚
â”‚ RAG Agent Fields:                   â”‚
â”‚ â”œâ”€ required_skills (JSON)           â”‚
â”‚ â”œâ”€ preferred_skills (JSON)          â”‚
â”‚ â”œâ”€ domain_requirements (JSON)       â”‚
â”‚ â”œâ”€ dynamic_evaluation_criteria (J)  â”‚
â”‚ â”œâ”€ competency_weights (JSON)        â”‚
â”‚ â”œâ”€ weights_reasoning (JSON)         â”‚
â”‚ â”œâ”€ position_type (VARCHAR)          â”‚
â”‚ â”œâ”€ seniority_level (VARCHAR)        â”‚
â”‚ â””â”€ main_responsibilities (JSON)     â”‚
â”‚                                     â”‚
â”‚ created_at, updated_at              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ 1:N
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       job_chunks (ì²­í¬)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                             â”‚
â”‚ job_id (FK â†’ jobs.id) CASCADE       â”‚
â”‚ chunk_text (TEXT)                   â”‚
â”‚ embedding (Vector(1024))  â† pgvectorâ”‚
â”‚ chunk_index (INTEGER)               â”‚
â”‚ created_at                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      jd_personas (í˜ë¥´ì†Œë‚˜)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                             â”‚
â”‚ job_id                              â”‚
â”‚ company_id                          â”‚
â”‚ company_name                        â”‚
â”‚                                     â”‚
â”‚ common_competencies (JSON)  â† 6 fixed
â”‚ job_competencies (JSON)     â† 6 extracted
â”‚ core_questions (JSON)       â† 3 company Q
â”‚ persona_summary (JSON)      â† 2 personas
â”‚                                     â”‚
â”‚ analysis_summary (TEXT)             â”‚
â”‚ visualization_config (JSON)         â”‚
â”‚ is_active (BOOLEAN)                 â”‚
â”‚ created_at, updated_at              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ 1:N
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   jd_persona_questions               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                             â”‚
â”‚ persona_id (FK â†’ jd_personas.id)    â”‚
â”‚ persona_type                        â”‚
â”‚ question_text                       â”‚
â”‚ question_category                   â”‚
â”‚ target_competencies (JSON)          â”‚
â”‚ is_active                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 5: Competency Mapping

### 5.1 Common Competencies (Fixed - 6 items)
```python
COMMON_COMPETENCIES = [
    "ê³ ê°ì§€í–¥",      # Customer orientation
    "ë„ì „ì •ì‹ ",      # Challenge spirit
    "í˜‘ë™",          # Cooperation
    "íŒ€ì›Œí¬",        # Teamwork
    "ëª©í‘œì§€í–¥",      # Goal orientation
    "ì±…ì„ê°"         # Sense of responsibility
]
```

### 5.2 Job-Specific Competencies (Extracted per JD - 6 items)
Example from Backend Developer JD:
```json
{
  "job_competencies": [
    "ë°ì´í„°ë¶„ì„",
    "ë¬¸ì œí•´ê²°ë ¥",
    "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
    "ì°½ì˜ì  ì‚¬ê³ ",
    "ê¸°ìˆ ì  ì´í•´",
    "ë¦¬ë”ì‹­"
  ]
}
```

### 5.3 Competency Weights (6 competencies total)
```python
{
  "job_expertise": 0.40,        # 40% - Job-specific technical skills
  "analytical": 0.15,           # 15% - Analytical thinking
  "execution": 0.20,            # 20% - Execution ability
  "relationship": 0.10,         # 10% - Team collaboration
  "resilience": 0.05,           # 5%  - Stress management
  "influence": 0.10              # 10% - Leadership/influence
}
```

### 5.4 Persona-to-Competency Mapping
```json
{
  "persona_summary": [
    {
      "type": "ë…¼ë¦¬í˜• ë©´ì ‘ê´€ (Analytical)",
      "focus": "ë¬¸ì œí•´ê²°ë ¥ê³¼ ë¶„ì„ì  ì‚¬ê³ ë¥¼ ì¤‘ì  í‰ê°€",
      "target_competencies": [
        "ë¶„ì„ì  ì‚¬ê³ ",      â† From job_competencies
        "ë¬¸ì œí•´ê²°ë ¥"        â† From job_competencies
      ],
      "example_question": "í”„ë¡œì íŠ¸ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜ìš”?"
    },
    {
      "type": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜í˜• ë©´ì ‘ê´€ (Collaborative)",
      "focus": "í˜‘ì—… ë° ì†Œí†µ ëŠ¥ë ¥ í‰ê°€",
      "target_competencies": [
        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",      â† From job_competencies
        "íŒ€ì›Œí¬"             â† From common_competencies
      ],
      "example_question": "ì˜ê²¬ ì¶©ëŒì´ ìˆì—ˆì„ ë•Œ, ì–´ë–»ê²Œ ì¡°ìœ¨í–ˆë‚˜ìš”?"
    }
  ]
}
```

---

## Part 6: Vector Search with pgvector

### 6.1 Embedding Generation
```python
# Uses Amazon Titan Text Embeddings V2
# Output: 1024-dimensional vector

chunk_text = "Backend ê°œë°œì í•„ìˆ˜ ìš”êµ¬ì‚¬í•­: Python, FastAPI, PostgreSQL..."
embedding = [0.0234, -0.0156, ..., 0.0891]  # 1024 dimensions
```

### 6.2 Vector Similarity Search
```python
# server/services/job_service.py
def search_similar_chunks(
    self,
    db: Session,
    query_text: str,
    top_k: int = 5,
    job_id: Optional[int] = None
) -> List[Dict[str, Any]]:
    from pgvector.sqlalchemy import cosine_distance

    # Generate query embedding
    query_embedding = self.embedding_service.generate_embedding(query_text)

    # Vector similarity search
    query = db.query(
        JobChunk.id,
        JobChunk.job_id,
        JobChunk.chunk_text,
        JobChunk.chunk_index,
        cosine_distance(JobChunk.embedding, query_embedding).label("distance")
    )

    if job_id:
        query = query.filter(JobChunk.job_id == job_id)

    results = query.order_by("distance").limit(top_k).all()

    return [
        {
            "chunk_id": r.id,
            "job_id": r.job_id,
            "chunk_text": r.chunk_text,
            "chunk_index": r.chunk_index,
            "similarity": 1 - r.distance  # Convert distance to similarity
        }
        for r in results
    ]
```

### 6.3 Query Example
```
Query: "Pythonê³¼ FastAPI ê²½í—˜"
â†“
Generate embedding (1024 dims)
â†“
Search job_chunks table using cosine_distance
â†“
Return top 5 most similar chunks with similarity scores
```

---

## Part 7: Error Handling and Fallbacks

### 7.1 RAG Agent Fallback
```python
def _get_default_parsed_data(self) -> Dict[str, Any]:
    """Fallback data if RAG Agent fails"""
    return {
        "required_skills": [],
        "preferred_skills": [],
        "domain_requirements": [],
        "dynamic_evaluation_criteria": [
            "ê¸°ìˆ  ì „ë¬¸ì„±",
            "ë¬¸ì œ í•´ê²° ëŠ¥ë ¥",
            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬",
            "íŒ€ì›Œí¬",
            "ì„±ì¥ ê°€ëŠ¥ì„±"
        ],
        "competency_weights": {
            "job_expertise": 0.30,
            "analytical": 0.15,
            "execution": 0.20,
            "relationship": 0.15,
            "resilience": 0.10,
            "influence": 0.10,
        },
        "position_type": "unknown",
        "seniority_level": "mid",
        "main_responsibilities": []
    }
```

### 7.2 Persona Question Parser Fallback
```python
def _extract_questions_fallback(self, pdf_text: str, company_name: str) -> Dict[str, Any]:
    """Regex-based fallback if LLM fails"""
    
    # Try regex patterns
    question_patterns = [
        r'^\d+\.\s+(.+?)(?=\n\d+\.|\Z)',  # "1. Question"
        r'^Q\d*[:)]\s+(.+?)(?=\nQ\d*[:)]|\Z)',  # "Q: Question"
        r'^\?\s+(.+?)(?=\n\?|\Z)',  # "? Question"
    ]
    
    questions = []
    for pattern in question_patterns:
        matches = re.finditer(pattern, pdf_text, re.MULTILINE | re.DOTALL)
        for match in matches:
            question_text = match.group(1).strip()
            if len(question_text) > 10:  # Filter short texts
                questions.append({
                    "question_text": question_text,
                    "question_type": "general",
                    "expected_keywords": [],
                    "evaluation_criteria": ["ë‹µë³€ì˜ ëª…í™•ì„±", "ë…¼ë¦¬ì  êµ¬ì¡°"],
                    "difficulty_level": 3
                })
    
    # If no questions found, create default
    if not questions:
        questions.append({
            "question_text": f"{company_name}ì—ì„œ ìš”êµ¬í•˜ëŠ” ì—­ëŸ‰ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "question_type": "general",
            "expected_keywords": [],
            "evaluation_criteria": ["ë‹µë³€ì˜ ëª…í™•ì„±", "ê²½í—˜ì˜ êµ¬ì²´ì„±"],
            "difficulty_level": 3
        })
    
    return {
        "persona_info": {
            "persona_name": f"{company_name} ë©´ì ‘ê´€",
            "archetype": "analytical",
            "description": f"{company_name}ì˜ ì±„ìš© ê¸°ì¤€ì„ í‰ê°€í•˜ëŠ” ë©´ì ‘ê´€",
            "focus_areas": ["ì—­ëŸ‰ í‰ê°€"]
        },
        "questions": questions
    }
```

---

## Part 8: API Integration Points

### 8.1 JD Upload Flow Sequence
```
1. Frontend (CompanyInfo.jsx)
   â””â”€> handleSave()
       â””â”€> POST /api/v1/jobs/upload (if job.pdf provided)

2. Backend (api/job.py)
   â””â”€> upload_jd_pdf()
       â””â”€> JobService.process_jd_pdf()

3. Job Service (services/job_service.py)
   â””â”€> Step 1: S3Service.upload_file()
   â””â”€> Step 2: JDParser.parse_and_chunk()
   â””â”€> Step 2-1: _extract_company_weights()
   â””â”€> Step 3: Create Job DB record
   â””â”€> Step 4: EmbeddingService.generate_embeddings_batch()
   â””â”€> Step 5: Create JobChunk DB records
   â””â”€> Return Job with chunks

4. Response back to Frontend
   â””â”€> JobResponse {job_id, company_id, title, created_at, total_chunks}
```

### 8.2 Persona Generation Flow Sequence
```
1. Frontend (CompanyInfo.jsx)
   â””â”€> handlePersonaUpload()
       â””â”€> POST /api/v1/jd-persona/upload
           â””â”€> (creates Job via JobService)
           â””â”€> CompetencyService.analyze_jd_competencies()

2. Competency Analysis (services/competency_service.py)
   â””â”€> analyze_jd_competencies()
       â””â”€> LLM call: extract 6 job-specific competencies

3. Persona Generation (api/jd_persona.py)
   â””â”€> POST /api/v1/jd-persona/generate-persona
       â””â”€> Request: {job_id, company_questions (3 items)}

4. JD Persona Service (services/jd_persona_service.py)
   â””â”€> create_and_save_persona()
       â””â”€> CompetencyService.analyze_jd_competencies()
       â””â”€> CompetencyService.generate_persona_data()
           â””â”€> LLM call: generate 2 personas
       â””â”€> get_competency_visualization_data()
       â””â”€> JDPersona.create_from_generation_result()
       â””â”€> Save to DB
       â””â”€> Return complete persona object

5. Response to Frontend
   â””â”€> PersonaResponse {job_id, company, competencies, personas, created_at}
```

